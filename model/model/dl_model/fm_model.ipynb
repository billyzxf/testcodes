{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a42f28cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "474afddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['phone_model', 'browser_family', 'os_family', 'device_brand', 'city_code', 'province_code',\n",
    "                        'sex', 'hashouse', 'social', 'overdue',\n",
    "                        'tax', 'married', 'benke', 'kid', 'income', 'consumption', 'shebao']\n",
    "\n",
    "text_features = ['clicked_products_0009', 'clicked_products_date_0009', 'sms_sent_products_0009',\n",
    "                 'sms_sent_products_1019', 'sms_sent_products_date_0009', 'sms_sent_products_date_1019',\n",
    "                 'called_products_0009', 'called_products_1019', 'called_products_date_0009',\n",
    "                 'called_products_date_1019', 'picked_products_0009', 'picked_products_date_0009',\n",
    "                 'outbound_sent_products_0009', 'outbound_sent_products_date_0009', 'set_all_ins_host_180', 'set_all_ins_host_360',] \\\n",
    "                + ['keypress_30', 'keypress_60', 'keypress_90', 'keypress_120',\n",
    "                   'rule_name_30', 'rule_name_60', 'rule_name_90', 'rule_name_120',\n",
    "                   'semantic_30', 'semantic_60', 'semantic_90', 'semantic_120', 'model_value']\n",
    "\n",
    "text_feature_types = ['products', 'keypress', 'rules', 'semantics', 'insurances', 'model_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e28fe500",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_excel(\"../../data/train_data_ifh4.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb51ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[text_features] = train_df[text_features].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa76d9bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clicked_products_0009</th>\n",
       "      <th>clicked_products_date_0009</th>\n",
       "      <th>sms_sent_products_0009</th>\n",
       "      <th>sms_sent_products_1019</th>\n",
       "      <th>sms_sent_products_date_0009</th>\n",
       "      <th>sms_sent_products_date_1019</th>\n",
       "      <th>called_products_0009</th>\n",
       "      <th>called_products_1019</th>\n",
       "      <th>called_products_date_0009</th>\n",
       "      <th>called_products_date_1019</th>\n",
       "      <th>...</th>\n",
       "      <th>keypress_120</th>\n",
       "      <th>rule_name_30</th>\n",
       "      <th>rule_name_60</th>\n",
       "      <th>rule_name_90</th>\n",
       "      <th>rule_name_120</th>\n",
       "      <th>semantic_30</th>\n",
       "      <th>semantic_60</th>\n",
       "      <th>semantic_90</th>\n",
       "      <th>semantic_120</th>\n",
       "      <th>model_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>...</td>\n",
       "      <td>小助理 运营商提示音</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>baotai27_其他</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>144 145 140 143</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>IYBPAZX_ZTKMF_OPPOJX_BT IYBPAZX_ZTKMF_OPPOJX_BT</td>\n",
       "      <td>nan</td>\n",
       "      <td>2D 4D</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>baotai27_G</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>7 9 140 150 151 7 131 131</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FQL IYBPAZX_ZMF_CS_BT NWZAZX_ZNWZAMF_NEWOPPO_B...</td>\n",
       "      <td>12D 12D 15D 24D 30D 32D 36D 38D 41D 44D</td>\n",
       "      <td>IYBPAZX_ZTKMF_OPPOJX_BT NWZAZX_ZNWZAMF_NEWOPPO...</td>\n",
       "      <td>IYBPAZX_ZTKMF_OPPOJX_BT IYBPAZX_ZMF_CS_BT IYBP...</td>\n",
       "      <td>2D 4D 8D 10D 12D 15D 20D 24D 26D 28D</td>\n",
       "      <td>30D 32D 34D 36D 39D 43D 45D 47D 51D 53D</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>...</td>\n",
       "      <td>触发发短信 sendMessage_special 没输入手机号 输入手机号2 输入手机号3...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>yingdian888_B jiyonghua661_B huirong888_D ying...</td>\n",
       "      <td>mayi02_D baotai14_B mayi02_D mayi02_D</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>2240 2572 2672 2313 2345 2672 2313 2313 2313 2...</td>\n",
       "      <td>23 24 46 3921 35 21 21 2576 2181 2189 24 23 23...</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>IYBPAZX_ZTKMF_OPPOJX_BT IYBPAZX_ZMF_CS_BT IYBP...</td>\n",
       "      <td>nan</td>\n",
       "      <td>2D 4D 6D</td>\n",
       "      <td>nan</td>\n",
       "      <td>IYBPAZX_TKMF_GD_BZ_GZH_WH IYBPAZX_TKMF_GD_BZ_G...</td>\n",
       "      <td>nan</td>\n",
       "      <td>1D 4D 7D 52D 55D 58D</td>\n",
       "      <td>nan</td>\n",
       "      <td>...</td>\n",
       "      <td>触发发短信 sendMessage_special</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>baotai27_D</td>\n",
       "      <td>baotai27_其他</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               clicked_products_0009  \\\n",
       "0                                                nan   \n",
       "1                                                nan   \n",
       "2                                                nan   \n",
       "3  FQL IYBPAZX_ZMF_CS_BT NWZAZX_ZNWZAMF_NEWOPPO_B...   \n",
       "4                                                nan   \n",
       "\n",
       "                clicked_products_date_0009  \\\n",
       "0                                      nan   \n",
       "1                                      nan   \n",
       "2                                      nan   \n",
       "3  12D 12D 15D 24D 30D 32D 36D 38D 41D 44D   \n",
       "4                                      nan   \n",
       "\n",
       "                              sms_sent_products_0009  \\\n",
       "0                                                nan   \n",
       "1    IYBPAZX_ZTKMF_OPPOJX_BT IYBPAZX_ZTKMF_OPPOJX_BT   \n",
       "2                                                nan   \n",
       "3  IYBPAZX_ZTKMF_OPPOJX_BT NWZAZX_ZNWZAMF_NEWOPPO...   \n",
       "4  IYBPAZX_ZTKMF_OPPOJX_BT IYBPAZX_ZMF_CS_BT IYBP...   \n",
       "\n",
       "                              sms_sent_products_1019  \\\n",
       "0                                                nan   \n",
       "1                                                nan   \n",
       "2                                                nan   \n",
       "3  IYBPAZX_ZTKMF_OPPOJX_BT IYBPAZX_ZMF_CS_BT IYBP...   \n",
       "4                                                nan   \n",
       "\n",
       "            sms_sent_products_date_0009  \\\n",
       "0                                   nan   \n",
       "1                                 2D 4D   \n",
       "2                                   nan   \n",
       "3  2D 4D 8D 10D 12D 15D 20D 24D 26D 28D   \n",
       "4                              2D 4D 6D   \n",
       "\n",
       "               sms_sent_products_date_1019  \\\n",
       "0                                      nan   \n",
       "1                                      nan   \n",
       "2                                      nan   \n",
       "3  30D 32D 34D 36D 39D 43D 45D 47D 51D 53D   \n",
       "4                                      nan   \n",
       "\n",
       "                                called_products_0009 called_products_1019  \\\n",
       "0                                                nan                  nan   \n",
       "1                                                nan                  nan   \n",
       "2                                                nan                  nan   \n",
       "3                                                nan                  nan   \n",
       "4  IYBPAZX_TKMF_GD_BZ_GZH_WH IYBPAZX_TKMF_GD_BZ_G...                  nan   \n",
       "\n",
       "  called_products_date_0009 called_products_date_1019  ...  \\\n",
       "0                       nan                       nan  ...   \n",
       "1                       nan                       nan  ...   \n",
       "2                       nan                       nan  ...   \n",
       "3                       nan                       nan  ...   \n",
       "4      1D 4D 7D 52D 55D 58D                       nan  ...   \n",
       "\n",
       "                                        keypress_120 rule_name_30  \\\n",
       "0                                         小助理 运营商提示音          nan   \n",
       "1                                                nan          nan   \n",
       "2                                                nan          nan   \n",
       "3  触发发短信 sendMessage_special 没输入手机号 输入手机号2 输入手机号3...          nan   \n",
       "4                          触发发短信 sendMessage_special          nan   \n",
       "\n",
       "  rule_name_60                                       rule_name_90  \\\n",
       "0          nan                                                nan   \n",
       "1          nan                                                nan   \n",
       "2          nan                                         baotai27_G   \n",
       "3          nan  yingdian888_B jiyonghua661_B huirong888_D ying...   \n",
       "4          nan                                         baotai27_D   \n",
       "\n",
       "                           rule_name_120 semantic_30 semantic_60  \\\n",
       "0                            baotai27_其他         nan         nan   \n",
       "1                                    nan         nan         nan   \n",
       "2                                    nan         nan         nan   \n",
       "3  mayi02_D baotai14_B mayi02_D mayi02_D         nan         nan   \n",
       "4                            baotai27_其他         nan         nan   \n",
       "\n",
       "                                         semantic_90  \\\n",
       "0                                                nan   \n",
       "1                                                nan   \n",
       "2                          7 9 140 150 151 7 131 131   \n",
       "3  2240 2572 2672 2313 2345 2672 2313 2313 2313 2...   \n",
       "4                                                nan   \n",
       "\n",
       "                                        semantic_120 model_value  \n",
       "0                                    144 145 140 143         nan  \n",
       "1                                                nan         nan  \n",
       "2                                                nan         nan  \n",
       "3  23 24 46 3921 35 21 21 2576 2181 2189 24 23 23...         nan  \n",
       "4                                                nan         nan  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[text_features].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1b3a5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(feature_type):\n",
    "    words = train_df[feature_type].dropna().str.split(' ')\n",
    "    exploded_words = words.explode()\n",
    "    vocabulary = exploded_words.value_counts()\n",
    "    vocabulary = vocabulary[vocabulary > 5]\n",
    "    vocab_size = vocabulary.shape[0]\n",
    "    vocabulary = vocabulary.index.tolist()\n",
    "\n",
    "    return vocabulary, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b542d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {}\n",
    "for type in text_feature_types:\n",
    "    vocab_dict[type] = get_vocabulary(type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99e7e417",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_dict = {col: vocab_info[1]+100 for col, vocab_info in vocab_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9818728c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'products': 273,\n",
       " 'keypress': 207,\n",
       " 'rules': 328,\n",
       " 'semantics': 351,\n",
       " 'insurances': 141,\n",
       " 'model_value': 102}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868d7ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only first time\n",
    "import pickle\n",
    "\n",
    "\n",
    "max_processes = 5\n",
    "oov_tok = '<OOV>'\n",
    "\n",
    "\n",
    "def get_token(text_feature_type):\n",
    "    print(\"fitting tokenizer:\", text_feature_type)\n",
    "    token = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size_dict[text_feature_type],\n",
    "                                                  oov_token=oov_tok, filters='')\n",
    "    token.fit_on_texts(train_df[text_feature_type].fillna(\"\").astype(str))\n",
    "    return {text_feature_type: token}\n",
    "\n",
    "\n",
    "result_list = [get_token(t) for t in text_feature_types]\n",
    "result_dict = {}\n",
    "for d in result_list:\n",
    "    result_dict.update(d)\n",
    "\n",
    "token_dict = {}\n",
    "\n",
    "for text_feature_type in text_feature_types:\n",
    "    token_dict[text_feature_type] = result_dict[text_feature_type]\n",
    "\n",
    "with open(f'../../data/dicts/token_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(token_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd701547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'../../data/dicts/token_dict.pkl', 'rb') as f:\n",
    "    token_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9e60349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_dict = {\n",
    "    'called_products_0009': 10,\n",
    "    'called_products_1019': 6,\n",
    "    'called_products_date_0009': 10,\n",
    "    'called_products_date_1019': 6,\n",
    "    'clicked_products_0009': 3,\n",
    "    'clicked_products_date_0009': 3,\n",
    "    'keypress_120': 3,\n",
    "    'keypress_30': 3,\n",
    "    'keypress_60': 3,\n",
    "    'keypress_90': 3,\n",
    "    'label_0009': 3,\n",
    "    'label_1019': 3,\n",
    "    'label_date_0009': 3,\n",
    "    'label_date_1019': 3,\n",
    "    'model_value': 3,\n",
    "    'outbound_sent_products_0009': 3,\n",
    "    'outbound_sent_products_date_0009': 3,\n",
    "    'picked_products_0009': 3,\n",
    "    'picked_products_date_0009': 3,\n",
    "    'rule_name_120': 3,\n",
    "    'rule_name_30': 3,\n",
    "    'rule_name_60': 3,\n",
    "    'rule_name_90': 3,\n",
    "    'semantic_120': 5,\n",
    "    'semantic_30': 3,\n",
    "    'semantic_60': 3,\n",
    "    'semantic_90': 5,\n",
    "    'set_all_ins_host_180': 5,\n",
    "    'set_all_ins_host_360': 4,\n",
    "    'sms_sent_products_0009': 10,\n",
    "    'sms_sent_products_1019': 9,\n",
    "    'sms_sent_products_date_0009': 10,\n",
    "    'sms_sent_products_date_1019': 9\n",
    " }\n",
    "\n",
    "category_counts_dict = {col: train_df[col].nunique() for col in categorical_features}\n",
    "\n",
    "\n",
    "def get_text_feature_type(col):\n",
    "    if 'set_' in col:\n",
    "        text_feature_type = 'insurances'\n",
    "    elif 'host_' in col:\n",
    "        text_feature_type = 'hosts'\n",
    "    elif 'products_' in col:\n",
    "        text_feature_type = 'products'\n",
    "    elif 'label_' in col:\n",
    "        text_feature_type = 'labels'\n",
    "    elif 'keypress_' in col:\n",
    "        text_feature_type = 'keypress'\n",
    "    elif 'rule_name_' in col:\n",
    "        text_feature_type = 'rules'\n",
    "    elif 'semantic_' in col:\n",
    "        text_feature_type = 'semantics'\n",
    "    else:\n",
    "        text_feature_type = 'model_value'\n",
    "    return text_feature_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419c2c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tokenize : clicked_products_0009\n",
      " tokenize : clicked_products_date_0009\n",
      " tokenize : sms_sent_products_0009\n",
      " tokenize : sms_sent_products_1019\n",
      " tokenize : sms_sent_products_date_0009\n",
      " tokenize : sms_sent_products_date_1019\n",
      " tokenize : called_products_0009\n",
      " tokenize : called_products_1019\n",
      " tokenize : called_products_date_0009\n",
      " tokenize : called_products_date_1019\n",
      " tokenize : picked_products_0009\n",
      " tokenize : picked_products_date_0009\n",
      " tokenize : outbound_sent_products_0009\n",
      " tokenize : outbound_sent_products_date_0009\n",
      " tokenize : set_all_ins_host_180\n",
      " tokenize : set_all_ins_host_360\n",
      " tokenize : keypress_30\n",
      " tokenize : keypress_60\n",
      " tokenize : keypress_90\n",
      " tokenize : keypress_120\n",
      " tokenize : rule_name_30\n",
      " tokenize : rule_name_60\n",
      " tokenize : rule_name_90\n",
      " tokenize : rule_name_120\n",
      " tokenize : semantic_30\n",
      " tokenize : semantic_60\n",
      " tokenize : semantic_90\n",
      " tokenize : semantic_120\n",
      " tokenize : model_value\n"
     ]
    }
   ],
   "source": [
    "# tokenization and padding\n",
    "padding_type = 'post'\n",
    "truncate_type = 'post'\n",
    "\n",
    "\n",
    "def tokenize(col):\n",
    "    print(\" tokenize :\", col)\n",
    "    text_feature_type = get_text_feature_type(col)\n",
    "    token = token_dict[text_feature_type]\n",
    "    max_len = max_len_dict[col]\n",
    "    tokenized_seq = token.texts_to_sequences(train_df[col])\n",
    "    result_train = tf.keras.preprocessing.sequence.pad_sequences(tokenized_seq, maxlen=max_len, padding=padding_type,\n",
    "                                                                 truncating=truncate_type)\n",
    "    with open(f'../../data/text_features/{col}.pkl', 'wb') as f:\n",
    "        pickle.dump(result_train, f)\n",
    "\n",
    "for col in text_features:\n",
    "    tokenize(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "77afc4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore numerical features\n",
    "restored_raw = (\n",
    "    train_df[\"numerical_features\"]\n",
    "    .astype(str)\n",
    "    .str.replace(\"\\n\", \" \", regex=False)\n",
    "    .str.strip(\"[]\")\n",
    "    .str.strip()\n",
    "    .str.split(r\"\\s+\", expand=True)\n",
    "    .apply(lambda col: pd.to_numeric(col.replace('', pd.NA), errors='coerce'))\n",
    "    .fillna(0)\n",
    "    .to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "131b7ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_train_cro = train_df['label'].str.strip(\"[]\").str.split(\" \", expand=True).astype(int).to_numpy()\n",
    "\n",
    "# X\n",
    "X_train = {'numerical_features': restored_raw}\n",
    "\n",
    "for col in categorical_features:\n",
    "    X_train[col] = np.array(train_df[col])\n",
    "\n",
    "for col in text_features:\n",
    "    with open(f'../../data/text_features/{col}.pkl', 'rb') as f:\n",
    "        X_train[col] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e6bb553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 拆分索引\n",
    "indices = np.arange(len(y_train_cro))\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42, stratify=y_train_cro)\n",
    "\n",
    "# 拆分 y\n",
    "y_train_split = y_train_cro[train_idx]\n",
    "y_test_split = y_train_cro[test_idx]\n",
    "\n",
    "# 拆分 X\n",
    "X_train_split = {}\n",
    "X_test_split = {}\n",
    "for key in X_train:\n",
    "    arr = X_train[key]\n",
    "    X_train_split[key] = arr[train_idx]\n",
    "    X_test_split[key] = arr[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ddcc13a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_dim=1, k=5):\n",
    "        super(FMLayer, self).__init__()\n",
    "        self.k = k\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = int(input_shape[-1])\n",
    "        self.b = self.add_weight(name='b', shape=(1,), initializer='zeros', trainable=True)\n",
    "        self.w = self.add_weight(name='w', shape=(input_dim, self.output_dim), initializer='glorot_normal', trainable=True)\n",
    "        self.v = self.add_weight(name='v', shape=(input_dim, self.k), initializer='glorot_normal', trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        linear_part = tf.matmul(inputs, self.w) + self.b   #shape:(batchsize, 1)\n",
    "\n",
    "        square_of_sum = tf.pow(tf.matmul(inputs, self.v), 2)  #shape:(batchsize, k)\n",
    "        sum_of_square = tf.matmul(tf.pow(inputs, 2), tf.pow(self.v, 2)) #shape:(batchsize, k)\n",
    "        inter_part = 0.5*tf.reduce_sum(square_of_sum - sum_of_square, axis=-1, keepdims=True) #shape:(batchsize, 1)\n",
    "\n",
    "        output = linear_part + inter_part\n",
    "        return output # shape:(batchsize, self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a68bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fm_model(output_unit, l2_reg_text_embedding=0.0, l2_reg_categorical_embedding=0.0, dropout_rate_fm_logit=0.1):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # text inputs\n",
    "    text_inputs = []\n",
    "    for col in text_features:\n",
    "        text_inputs.append(tf.keras.layers.Input(shape=(max_len_dict[col],), name=col))\n",
    "    text_embeddings = []\n",
    "    for i, col in enumerate(text_features):\n",
    "        text_feature_type = get_text_feature_type(col)\n",
    "        text_embeddings.append(\n",
    "            tf.keras.layers.Embedding(\n",
    "                vocab_size_dict[text_feature_type]+1, \n",
    "                int(np.log1p(vocab_size_dict[text_feature_type])+2),\n",
    "                embeddings_regularizer=tf.keras.regularizers.l2(l2_reg_text_embedding), \n",
    "                name=col + '_embed')(text_inputs[i])\n",
    "        )\n",
    "\n",
    "    text_logit = tf.keras.layers.Concatenate(name='text_concat')(\n",
    "        [tf.keras.layers.GlobalAveragePooling1D()(text_emb) for text_emb in text_embeddings]\n",
    "    )\n",
    "\n",
    "    # categorical inputs\n",
    "    categorical_inputs = [tf.keras.layers.Input(shape=(1,), name=col) for col in categorical_features]\n",
    "    categorical_embeddings = []\n",
    "    for i, col in enumerate(categorical_features):\n",
    "        categorical_embeddings.append(\n",
    "            tf.keras.layers.Embedding(category_counts_dict[col]+2, int(np.log1p(category_counts_dict[col]) + 1),\n",
    "                                      embeddings_regularizer=tf.keras.regularizers.l2(l2_reg_categorical_embedding),\n",
    "                                      name=col + '_embed')(categorical_inputs[i])\n",
    "        )\n",
    "\n",
    "    categorical_logit = tf.keras.layers.Concatenate(name='categorical_concat')(\n",
    "        [tf.keras.layers.Flatten()(cat_emb) for cat_emb in categorical_embeddings]\n",
    "    )\n",
    "\n",
    "    # numerical inputs\n",
    "    numerical_input = tf.keras.layers.Input(shape=(230,), name='numerical_features')\n",
    "\n",
    "    # fm\n",
    "    fm_input_layer = FMLayer(k=5)  # 输出单个 logit (batch,1)\n",
    "    fm_concat = tf.keras.layers.Concatenate(name='fm_concat')([text_logit, categorical_logit, numerical_input])\n",
    "    fm_logit = fm_input_layer(fm_concat)  # (batch,1)\n",
    "    fm_logit = tf.keras.layers.Dropout(dropout_rate_fm_logit)(fm_logit)\n",
    "    fm_logit = tf.keras.layers.Flatten()(fm_logit)\n",
    "\n",
    "    # output\n",
    "    outputs = tf.keras.layers.Dense(output_unit, activation='sigmoid')(fm_logit)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=text_inputs + categorical_inputs + [numerical_input], outputs=outputs, name='base_model')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9cc2dbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "n_epochs = 100\n",
    "batch_size = 1024\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.4, patience=1, min_lr=0.00005, verbose=1)\n",
    "callbacks = [es, reduce_lr]\n",
    "\n",
    "\n",
    "def train_model(x_train, y_train, x_test, y_test):\n",
    "    output_units = 3\n",
    "    model = base_model(output_units)\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy(name='acc'), tf.keras.metrics.AUC(name='auc')],\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr))\n",
    "    \n",
    "    model.fit(x_train, y_train, validation_data=(x_test, y_test),\n",
    "              epochs=n_epochs, batch_size=batch_size, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b7f28d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 326ms/step - acc: 0.5725 - auc: 0.5629 - loss: 0.9212 - val_acc: 0.5349 - val_auc: 0.5830 - val_loss: 0.8539 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - acc: 0.5854 - auc: 0.5822 - loss: 0.8957 - val_acc: 0.5462 - val_auc: 0.6051 - val_loss: 0.8266 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - acc: 0.5925 - auc: 0.6031 - loss: 0.8657 - val_acc: 0.5626 - val_auc: 0.6272 - val_loss: 0.8016 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - acc: 0.6056 - auc: 0.6216 - loss: 0.8401 - val_acc: 0.5770 - val_auc: 0.6482 - val_loss: 0.7788 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - acc: 0.6148 - auc: 0.6386 - loss: 0.8093 - val_acc: 0.5880 - val_auc: 0.6682 - val_loss: 0.7583 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - acc: 0.6267 - auc: 0.6561 - loss: 0.7923 - val_acc: 0.6025 - val_auc: 0.6859 - val_loss: 0.7394 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - acc: 0.6361 - auc: 0.6750 - loss: 0.7679 - val_acc: 0.6132 - val_auc: 0.7017 - val_loss: 0.7224 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - acc: 0.6472 - auc: 0.6874 - loss: 0.7526 - val_acc: 0.6237 - val_auc: 0.7168 - val_loss: 0.7068 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - acc: 0.6586 - auc: 0.7046 - loss: 0.7337 - val_acc: 0.6364 - val_auc: 0.7313 - val_loss: 0.6926 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - acc: 0.6728 - auc: 0.7192 - loss: 0.7137 - val_acc: 0.6479 - val_auc: 0.7432 - val_loss: 0.6795 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - acc: 0.6850 - auc: 0.7313 - loss: 0.7041 - val_acc: 0.6613 - val_auc: 0.7555 - val_loss: 0.6673 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - acc: 0.6951 - auc: 0.7466 - loss: 0.6885 - val_acc: 0.6783 - val_auc: 0.7673 - val_loss: 0.6560 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - acc: 0.7095 - auc: 0.7601 - loss: 0.6727 - val_acc: 0.6964 - val_auc: 0.7791 - val_loss: 0.6452 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - acc: 0.7241 - auc: 0.7867 - loss: 0.6662 - val_acc: 0.7179 - val_auc: 0.7890 - val_loss: 0.6350 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - acc: 0.7434 - auc: 0.8013 - loss: 0.6463 - val_acc: 0.7374 - val_auc: 0.7975 - val_loss: 0.6254 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - acc: 0.7599 - auc: 0.8106 - loss: 0.6409 - val_acc: 0.7579 - val_auc: 0.8061 - val_loss: 0.6163 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - acc: 0.7781 - auc: 0.8189 - loss: 0.6248 - val_acc: 0.7846 - val_auc: 0.8142 - val_loss: 0.6076 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - acc: 0.8026 - auc: 0.8268 - loss: 0.6189 - val_acc: 0.8034 - val_auc: 0.8229 - val_loss: 0.5992 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - acc: 0.8237 - auc: 0.8347 - loss: 0.6163 - val_acc: 0.8174 - val_auc: 0.8303 - val_loss: 0.5911 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - acc: 0.8389 - auc: 0.8432 - loss: 0.6067 - val_acc: 0.8342 - val_auc: 0.8393 - val_loss: 0.5835 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - acc: 0.8528 - auc: 0.8498 - loss: 0.5976 - val_acc: 0.8501 - val_auc: 0.8472 - val_loss: 0.5762 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - acc: 0.8636 - auc: 0.8551 - loss: 0.5879 - val_acc: 0.8604 - val_auc: 0.8535 - val_loss: 0.5692 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - acc: 0.8761 - auc: 0.8628 - loss: 0.5769 - val_acc: 0.8689 - val_auc: 0.8588 - val_loss: 0.5623 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - acc: 0.8840 - auc: 0.8688 - loss: 0.5703 - val_acc: 0.8753 - val_auc: 0.8640 - val_loss: 0.5557 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - acc: 0.8886 - auc: 0.8719 - loss: 0.5661 - val_acc: 0.8812 - val_auc: 0.8691 - val_loss: 0.5493 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - acc: 0.8952 - auc: 0.8771 - loss: 0.5615 - val_acc: 0.8871 - val_auc: 0.8725 - val_loss: 0.5430 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - acc: 0.9003 - auc: 0.8823 - loss: 0.5532 - val_acc: 0.8904 - val_auc: 0.8772 - val_loss: 0.5369 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - acc: 0.9036 - auc: 0.8862 - loss: 0.5428 - val_acc: 0.8942 - val_auc: 0.8810 - val_loss: 0.5310 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - acc: 0.9053 - auc: 0.8892 - loss: 0.5384 - val_acc: 0.8973 - val_auc: 0.8849 - val_loss: 0.5251 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - acc: 0.9087 - auc: 0.8939 - loss: 0.5294 - val_acc: 0.9002 - val_auc: 0.8894 - val_loss: 0.5194 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - acc: 0.9111 - auc: 0.8975 - loss: 0.5222 - val_acc: 0.9036 - val_auc: 0.8939 - val_loss: 0.5139 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - acc: 0.9119 - auc: 0.8989 - loss: 0.5175 - val_acc: 0.9056 - val_auc: 0.8977 - val_loss: 0.5084 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - acc: 0.9142 - auc: 0.9019 - loss: 0.5151 - val_acc: 0.9064 - val_auc: 0.8996 - val_loss: 0.5031 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - acc: 0.9165 - auc: 0.9062 - loss: 0.5068 - val_acc: 0.9086 - val_auc: 0.9028 - val_loss: 0.4979 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - acc: 0.9183 - auc: 0.9067 - loss: 0.5062 - val_acc: 0.9103 - val_auc: 0.9056 - val_loss: 0.4928 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - acc: 0.9203 - auc: 0.9122 - loss: 0.4942 - val_acc: 0.9124 - val_auc: 0.9084 - val_loss: 0.4879 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - acc: 0.9217 - auc: 0.9138 - loss: 0.4904 - val_acc: 0.9132 - val_auc: 0.9099 - val_loss: 0.4830 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - acc: 0.9236 - auc: 0.9159 - loss: 0.4856 - val_acc: 0.9152 - val_auc: 0.9120 - val_loss: 0.4782 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - acc: 0.9245 - auc: 0.9164 - loss: 0.4828 - val_acc: 0.9169 - val_auc: 0.9150 - val_loss: 0.4735 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - acc: 0.9264 - auc: 0.9191 - loss: 0.4768 - val_acc: 0.9191 - val_auc: 0.9175 - val_loss: 0.4689 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - acc: 0.9276 - auc: 0.9206 - loss: 0.4714 - val_acc: 0.9198 - val_auc: 0.9200 - val_loss: 0.4644 - learning_rate: 1.0000e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - acc: 0.9285 - auc: 0.9212 - loss: 0.4680 - val_acc: 0.9223 - val_auc: 0.9220 - val_loss: 0.4600 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - acc: 0.9290 - auc: 0.9222 - loss: 0.4634 - val_acc: 0.9247 - val_auc: 0.9232 - val_loss: 0.4556 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - acc: 0.9305 - auc: 0.9244 - loss: 0.4591 - val_acc: 0.9264 - val_auc: 0.9245 - val_loss: 0.4514 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - acc: 0.9327 - auc: 0.9268 - loss: 0.4533 - val_acc: 0.9279 - val_auc: 0.9263 - val_loss: 0.4472 - learning_rate: 1.0000e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - acc: 0.9341 - auc: 0.9296 - loss: 0.4486 - val_acc: 0.9286 - val_auc: 0.9286 - val_loss: 0.4431 - learning_rate: 1.0000e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - acc: 0.9347 - auc: 0.9299 - loss: 0.4481 - val_acc: 0.9298 - val_auc: 0.9298 - val_loss: 0.4391 - learning_rate: 1.0000e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - acc: 0.9361 - auc: 0.9307 - loss: 0.4433 - val_acc: 0.9305 - val_auc: 0.9301 - val_loss: 0.4351 - learning_rate: 1.0000e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - acc: 0.9373 - auc: 0.9311 - loss: 0.4380 - val_acc: 0.9313 - val_auc: 0.9313 - val_loss: 0.4313 - learning_rate: 1.0000e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - acc: 0.9379 - auc: 0.9333 - loss: 0.4352 - val_acc: 0.9321 - val_auc: 0.9320 - val_loss: 0.4274 - learning_rate: 1.0000e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - acc: 0.9381 - auc: 0.9340 - loss: 0.4307 - val_acc: 0.9327 - val_auc: 0.9328 - val_loss: 0.4237 - learning_rate: 1.0000e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - acc: 0.9388 - auc: 0.9339 - loss: 0.4280 - val_acc: 0.9335 - val_auc: 0.9340 - val_loss: 0.4201 - learning_rate: 1.0000e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - acc: 0.9395 - auc: 0.9353 - loss: 0.4248 - val_acc: 0.9337 - val_auc: 0.9349 - val_loss: 0.4165 - learning_rate: 1.0000e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - acc: 0.9404 - auc: 0.9365 - loss: 0.4215 - val_acc: 0.9342 - val_auc: 0.9353 - val_loss: 0.4129 - learning_rate: 1.0000e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - acc: 0.9407 - auc: 0.9358 - loss: 0.4197 - val_acc: 0.9345 - val_auc: 0.9360 - val_loss: 0.4094 - learning_rate: 1.0000e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - acc: 0.9414 - auc: 0.9378 - loss: 0.4136 - val_acc: 0.9352 - val_auc: 0.9371 - val_loss: 0.4061 - learning_rate: 1.0000e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - acc: 0.9416 - auc: 0.9386 - loss: 0.4110 - val_acc: 0.9359 - val_auc: 0.9378 - val_loss: 0.4027 - learning_rate: 1.0000e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - acc: 0.9428 - auc: 0.9393 - loss: 0.4062 - val_acc: 0.9367 - val_auc: 0.9383 - val_loss: 0.3994 - learning_rate: 1.0000e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - acc: 0.9421 - auc: 0.9397 - loss: 0.4048 - val_acc: 0.9384 - val_auc: 0.9390 - val_loss: 0.3961 - learning_rate: 1.0000e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - acc: 0.9428 - auc: 0.9397 - loss: 0.4026 - val_acc: 0.9394 - val_auc: 0.9391 - val_loss: 0.3930 - learning_rate: 1.0000e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - acc: 0.9445 - auc: 0.9395 - loss: 0.4003 - val_acc: 0.9396 - val_auc: 0.9401 - val_loss: 0.3899 - learning_rate: 1.0000e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - acc: 0.9443 - auc: 0.9410 - loss: 0.3944 - val_acc: 0.9396 - val_auc: 0.9410 - val_loss: 0.3868 - learning_rate: 1.0000e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - acc: 0.9445 - auc: 0.9394 - loss: 0.3921 - val_acc: 0.9399 - val_auc: 0.9418 - val_loss: 0.3838 - learning_rate: 1.0000e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - acc: 0.9446 - auc: 0.9411 - loss: 0.3921 - val_acc: 0.9406 - val_auc: 0.9426 - val_loss: 0.3808 - learning_rate: 1.0000e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - acc: 0.9451 - auc: 0.9421 - loss: 0.3876 - val_acc: 0.9409 - val_auc: 0.9426 - val_loss: 0.3779 - learning_rate: 1.0000e-04\n",
      "Epoch 66/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - acc: 0.9463 - auc: 0.9418 - loss: 0.3853 - val_acc: 0.9418 - val_auc: 0.9429 - val_loss: 0.3751 - learning_rate: 1.0000e-04\n",
      "Epoch 67/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - acc: 0.9464 - auc: 0.9427 - loss: 0.3825 - val_acc: 0.9428 - val_auc: 0.9436 - val_loss: 0.3723 - learning_rate: 1.0000e-04\n",
      "Epoch 68/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - acc: 0.9464 - auc: 0.9436 - loss: 0.3804 - val_acc: 0.9433 - val_auc: 0.9438 - val_loss: 0.3696 - learning_rate: 1.0000e-04\n",
      "Epoch 69/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - acc: 0.9479 - auc: 0.9438 - loss: 0.3780 - val_acc: 0.9433 - val_auc: 0.9444 - val_loss: 0.3669 - learning_rate: 1.0000e-04\n",
      "Epoch 70/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - acc: 0.9475 - auc: 0.9446 - loss: 0.3737 - val_acc: 0.9437 - val_auc: 0.9441 - val_loss: 0.3642 - learning_rate: 1.0000e-04\n",
      "Epoch 71/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - acc: 0.9475 - auc: 0.9439 - loss: 0.3722 - val_acc: 0.9437 - val_auc: 0.9440 - val_loss: 0.3616 - learning_rate: 1.0000e-04\n",
      "Epoch 72/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - acc: 0.9486 - auc: 0.9440 - loss: 0.3730 - val_acc: 0.9440 - val_auc: 0.9444 - val_loss: 0.3591 - learning_rate: 1.0000e-04\n",
      "Epoch 73/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - acc: 0.9484 - auc: 0.9456 - loss: 0.3667 - val_acc: 0.9440 - val_auc: 0.9447 - val_loss: 0.3566 - learning_rate: 1.0000e-04\n",
      "Epoch 74/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - acc: 0.9489 - auc: 0.9450 - loss: 0.3656 - val_acc: 0.9442 - val_auc: 0.9454 - val_loss: 0.3541 - learning_rate: 1.0000e-04\n",
      "Epoch 75/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - acc: 0.9490 - auc: 0.9461 - loss: 0.3637 - val_acc: 0.9443 - val_auc: 0.9449 - val_loss: 0.3517 - learning_rate: 1.0000e-04\n",
      "Epoch 76/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - acc: 0.9491 - auc: 0.9460 - loss: 0.3605 - val_acc: 0.9443 - val_auc: 0.9453 - val_loss: 0.3494 - learning_rate: 1.0000e-04\n",
      "Epoch 77/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - acc: 0.9494 - auc: 0.9442 - loss: 0.3609 - val_acc: 0.9443 - val_auc: 0.9456 - val_loss: 0.3470 - learning_rate: 1.0000e-04\n",
      "Epoch 78/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - acc: 0.9497 - auc: 0.9466 - loss: 0.3563 - val_acc: 0.9447 - val_auc: 0.9460 - val_loss: 0.3448 - learning_rate: 1.0000e-04\n",
      "Epoch 79/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - acc: 0.9500 - auc: 0.9463 - loss: 0.3555 - val_acc: 0.9450 - val_auc: 0.9461 - val_loss: 0.3426 - learning_rate: 1.0000e-04\n",
      "Epoch 80/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - acc: 0.9502 - auc: 0.9465 - loss: 0.3541 - val_acc: 0.9450 - val_auc: 0.9460 - val_loss: 0.3404 - learning_rate: 1.0000e-04\n",
      "Epoch 81/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - acc: 0.9505 - auc: 0.9468 - loss: 0.3518 - val_acc: 0.9450 - val_auc: 0.9465 - val_loss: 0.3382 - learning_rate: 1.0000e-04\n",
      "Epoch 82/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - acc: 0.9503 - auc: 0.9464 - loss: 0.3505 - val_acc: 0.9455 - val_auc: 0.9466 - val_loss: 0.3361 - learning_rate: 1.0000e-04\n",
      "Epoch 83/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - acc: 0.9503 - auc: 0.9456 - loss: 0.3488 - val_acc: 0.9457 - val_auc: 0.9470 - val_loss: 0.3341 - learning_rate: 1.0000e-04\n",
      "Epoch 84/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - acc: 0.9506 - auc: 0.9469 - loss: 0.3455 - val_acc: 0.9457 - val_auc: 0.9471 - val_loss: 0.3320 - learning_rate: 1.0000e-04\n",
      "Epoch 85/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - acc: 0.9508 - auc: 0.9464 - loss: 0.3442 - val_acc: 0.9462 - val_auc: 0.9472 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 86/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - acc: 0.9512 - auc: 0.9463 - loss: 0.3445 - val_acc: 0.9464 - val_auc: 0.9475 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 87/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - acc: 0.9514 - auc: 0.9469 - loss: 0.3414 - val_acc: 0.9464 - val_auc: 0.9478 - val_loss: 0.3262 - learning_rate: 1.0000e-04\n",
      "Epoch 88/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - acc: 0.9516 - auc: 0.9478 - loss: 0.3400 - val_acc: 0.9467 - val_auc: 0.9477 - val_loss: 0.3243 - learning_rate: 1.0000e-04\n",
      "Epoch 89/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - acc: 0.9516 - auc: 0.9486 - loss: 0.3373 - val_acc: 0.9467 - val_auc: 0.9481 - val_loss: 0.3224 - learning_rate: 1.0000e-04\n",
      "Epoch 90/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - acc: 0.9518 - auc: 0.9479 - loss: 0.3370 - val_acc: 0.9475 - val_auc: 0.9484 - val_loss: 0.3206 - learning_rate: 1.0000e-04\n",
      "Epoch 91/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - acc: 0.9518 - auc: 0.9481 - loss: 0.3361 - val_acc: 0.9477 - val_auc: 0.9485 - val_loss: 0.3188 - learning_rate: 1.0000e-04\n",
      "Epoch 92/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - acc: 0.9517 - auc: 0.9499 - loss: 0.3302 - val_acc: 0.9484 - val_auc: 0.9485 - val_loss: 0.3170 - learning_rate: 1.0000e-04\n",
      "Epoch 93/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - acc: 0.9522 - auc: 0.9494 - loss: 0.3314 - val_acc: 0.9484 - val_auc: 0.9487 - val_loss: 0.3153 - learning_rate: 1.0000e-04\n",
      "Epoch 94/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - acc: 0.9519 - auc: 0.9482 - loss: 0.3298 - val_acc: 0.9484 - val_auc: 0.9489 - val_loss: 0.3136 - learning_rate: 1.0000e-04\n",
      "Epoch 95/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - acc: 0.9522 - auc: 0.9481 - loss: 0.3290 - val_acc: 0.9484 - val_auc: 0.9489 - val_loss: 0.3119 - learning_rate: 1.0000e-04\n",
      "Epoch 96/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - acc: 0.9522 - auc: 0.9485 - loss: 0.3296 - val_acc: 0.9489 - val_auc: 0.9488 - val_loss: 0.3103 - learning_rate: 1.0000e-04\n",
      "Epoch 97/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - acc: 0.9524 - auc: 0.9487 - loss: 0.3252 - val_acc: 0.9492 - val_auc: 0.9490 - val_loss: 0.3087 - learning_rate: 1.0000e-04\n",
      "Epoch 98/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - acc: 0.9525 - auc: 0.9485 - loss: 0.3255 - val_acc: 0.9492 - val_auc: 0.9492 - val_loss: 0.3071 - learning_rate: 1.0000e-04\n",
      "Epoch 99/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - acc: 0.9525 - auc: 0.9496 - loss: 0.3212 - val_acc: 0.9496 - val_auc: 0.9490 - val_loss: 0.3056 - learning_rate: 1.0000e-04\n",
      "Epoch 100/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - acc: 0.9525 - auc: 0.9488 - loss: 0.3212 - val_acc: 0.9496 - val_auc: 0.9491 - val_loss: 0.3041 - learning_rate: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "train_model(X_train_split, y_train_split, X_test_split, y_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b036965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c1fa41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
