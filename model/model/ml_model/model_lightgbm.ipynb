{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f938ee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.ml.feature as E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c867dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = 'C:/Users/DT-Liuxiangfei/.conda/envs/python38/python.exe'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'C:/Users/DT-Liuxiangfei/.conda/envs/python38/python.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4478675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HADOOP_HOME'] = 'C:/Users/DT-Liuxiangfei/Documents/TestCodes/model/data/hadoop-3.3.6'\n",
    "os.environ['hadoop.home.dir'] = 'C:/Users/DT-Liuxiangfei/Documents/TestCodes/model/data/hadoop-3.3.6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86376acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LocalTest\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a89cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征定义保持与原脚本一致\n",
    "n_host_cols_list = [\n",
    "    'fre_all_host_10', 'fre_all_host_20', 'fre_all_host_30',\n",
    "    'fre_ins_host_10', 'fre_ins_host_20', 'fre_ins_host_30',\n",
    "    'fre_loan_host_10', 'fre_loan_host_20', 'fre_loan_host_30',\n",
    "    'fre_credit_host_10', 'fre_credit_host_20', 'fre_credit_host_30',\n",
    "    'fre_car_host_10', 'fre_car_host_20', 'fre_car_host_30',\n",
    "    'fre_pro_host_10', 'fre_pro_host_20', 'fre_pro_host_30',\n",
    "    'fre_edu_host_10', 'fre_edu_host_20', 'fre_edu_host_30',\n",
    "    'fre_inv_host_10', 'fre_inv_host_20', 'fre_inv_host_30',\n",
    "    'fre_health_host_10', 'fre_health_host_20', 'fre_health_host_30',\n",
    "    'ins_fre_rate_10', 'ins_fre_rate_20', 'ins_fre_rate_30',\n",
    "    'loan_fre_rate_10', 'loan_fre_rate_20', 'loan_fre_rate_30',\n",
    "    'credit_fre_rate_10', 'credit_fre_rate_20', 'credit_fre_rate_30',\n",
    "    'car_fre_rate_10', 'car_fre_rate_20', 'car_fre_rate_30',\n",
    "    'pro_fre_rate_10', 'pro_fre_rate_20', 'pro_fre_rate_30',\n",
    "    'edu_fre_rate_10', 'edu_fre_rate_20', 'edu_fre_rate_30',\n",
    "    'inv_fre_rate_10', 'inv_fre_rate_20', 'inv_fre_rate_30',\n",
    "    'health_fre_rate_10', 'health_fre_rate_20', 'health_fre_rate_30',\n",
    "    'fre_all_host_avg_30', 'fre_all_host_sd_30', 'fre_all_host_cv_30',\n",
    "    'fre_ins_host_avg_30', 'fre_ins_host_sd_30', 'fre_ins_host_cv_30',\n",
    "    'fre_loan_host_avg_30', 'fre_loan_host_sd_30', 'fre_loan_host_cv_30',\n",
    "    'fre_credit_host_avg_30', 'fre_credit_host_sd_30', 'fre_credit_host_cv_30',\n",
    "    'fre_car_host_avg_30', 'fre_car_host_sd_30', 'fre_car_host_cv_30',\n",
    "    'fre_pro_host_avg_30', 'fre_pro_host_sd_30', 'fre_pro_host_cv_30',\n",
    "    'fre_edu_host_avg_30', 'fre_edu_host_sd_30', 'fre_edu_host_cv_30',\n",
    "    'fre_inv_host_avg_30', 'fre_inv_host_sd_30', 'fre_inv_host_cv_30',\n",
    "    'fre_health_host_avg_30', 'fre_health_host_sd_30', 'fre_health_host_cv_30',\n",
    "    'ins_fre_rate_avg_30', 'ins_fre_rate_sd_30', 'ins_fre_rate_cv_30',\n",
    "    'loan_fre_rate_avg_30', 'loan_fre_rate_sd_30', 'loan_fre_rate_cv_30',\n",
    "    'credit_fre_rate_avg_30', 'credit_fre_rate_sd_30', 'credit_fre_rate_cv_30',\n",
    "    'car_fre_rate_avg_30', 'car_fre_rate_sd_30', 'car_fre_rate_cv_30',\n",
    "    'pro_fre_rate_avg_30', 'pro_fre_rate_sd_30', 'pro_fre_rate_cv_30',\n",
    "    'edu_fre_rate_avg_30', 'edu_fre_rate_sd_30', 'edu_fre_rate_cv_30',\n",
    "    'inv_fre_rate_avg_30', 'inv_fre_rate_sd_30', 'inv_fre_rate_cv_30',\n",
    "    'health_fre_rate_avg_30', 'health_fre_rate_sd_30', 'health_fre_rate_cv_30'\n",
    "]\n",
    "\n",
    "numerical_features = ['send_insure_30', 'send_insure_60', 'send_insure_90', 'send_insure_180',\n",
    "                      'days_since_send_insure_date', 'send_loan_30', 'send_loan_60', 'send_loan_90', 'send_loan_180',\n",
    "                      'days_since_send_loan_date', 'click_insure_30', 'click_insure_60', 'click_insure_90',\n",
    "                      'click_insure_180', 'days_since_click_insure_date', 'click_loan_30', 'click_loan_60',\n",
    "                      'click_loan_90', 'click_loan_180', 'days_since_click_loan_date', 'call_insure_30',\n",
    "                      'call_insure_60', 'call_insure_90', 'call_insure_180', 'days_since_call_insure_date',\n",
    "                      'call_loan_30', 'call_loan_60', 'call_loan_90', 'call_loan_180', 'days_since_call_loan_date',\n",
    "                      'pick_insure_30', 'pick_insure_60', 'pick_insure_90', 'pick_insure_180',\n",
    "                      'days_since_pick_insure_date', 'pick_loan_30', 'pick_loan_60', 'pick_loan_90', 'pick_loan_180',\n",
    "                      'days_since_pick_loan_date', 'call_send_insure_30', 'call_send_insure_60', 'call_send_insure_90',\n",
    "                      'call_send_insure_180', 'days_since_call_send_insure_date', 'call_send_loan_30',\n",
    "                      'call_send_loan_60', 'call_send_loan_90', 'call_send_loan_180', 'days_since_call_send_loan_date',\n",
    "                      'call_click_insure_30', 'call_click_insure_60', 'call_click_insure_90', 'call_click_insure_180',\n",
    "                      'days_since_call_click_insure_date', 'call_click_loan_30', 'call_click_loan_60',\n",
    "                      'call_click_loan_90', 'call_click_loan_180', 'days_since_call_click_loan_date', ] \\\n",
    "                     + ['city_level', 'new_age', 'active_duration', 'avg_price', \n",
    "                        'MAYI_OFFLINE_bx_32140', 'MAYI_OFFLINE_bx_33118', 'MAYI_OFFLINE_bx_32017', 'PROJECT_HS_bx_bx01', \n",
    "                        'MAYIYXD_GENERAL_bx_18184', 'MAYIYXD_GENERAL_bx_6134', 'PROJECT_TT_BX_EC_FLOW_RISK_ALIYUN_V21', 'MAYIYXD_GENERAL_bx_18825'] \\\n",
    "                      + n_host_cols_list \\\n",
    "                     + ['duration_30', 'duration_60', 'duration_90', 'duration_120',\n",
    "                        'interact_30', 'interact_60', 'interact_90', 'interact_120',\n",
    "                        'say_count_30', 'say_count_60', 'say_count_90', 'say_count_120',\n",
    "                        'freq_hangup_1_30', 'freq_hangup_1_60', 'freq_hangup_1_90', 'freq_hangup_1_120',\n",
    "                        'freq_hangup_2_30', 'freq_hangup_2_60', 'freq_hangup_2_90', 'freq_hangup_2_120',\n",
    "                        'hangup_1_prob_30', 'hangup_1_prob_60', 'hangup_1_prob_90', 'hangup_1_prob_120', ]\n",
    "categorical_features = ['phone_model', 'browser_family', 'os_family', 'device_brand', 'city_code', 'province_code',\n",
    "                        'sex', 'hashouse', 'social', 'overdue',\n",
    "                        'tax', 'married', 'benke', 'kid', 'income', 'consumption', 'shebao']\n",
    "text_features = ['host_0009', 'host_1019', 'host_2029', 'host_date_0009', 'host_date_1019', 'host_date_2029',\n",
    "                 'host_fre_0009', 'host_fre_1019', 'host_fre_2029', 'label_0009', 'label_1019', 'label_date_0009',\n",
    "                 'label_date_1019', 'clicked_products_0009', 'clicked_products_date_0009', 'sms_sent_products_0009',\n",
    "                 'sms_sent_products_1019', 'sms_sent_products_date_0009', 'sms_sent_products_date_1019',\n",
    "                 'called_products_0009', 'called_products_1019', 'called_products_date_0009',\n",
    "                 'called_products_date_1019', 'picked_products_0009', 'picked_products_date_0009',\n",
    "                 'outbound_sent_products_0009', 'outbound_sent_products_date_0009', ] \\\n",
    "                + ['keypress_30', 'keypress_60', 'keypress_90', 'keypress_120',\n",
    "                   'rule_name_30', 'rule_name_60', 'rule_name_90', 'rule_name_120',\n",
    "                   'semantic_30', 'semantic_60', 'semantic_90', 'semantic_120', ] + ['model_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8965893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据加载和预处理\n",
    "df = pd.read_excel(\"data/train_data.xlsx\")\n",
    "spark_df = spark.createDataFrame(df)\n",
    "label_data_new = spark_df.drop(\"hosts\", \"products\", \"labels\", \"keypress\", \"rules\", \"semantics\")\n",
    "\n",
    "for col in text_features:\n",
    "    label_data_new = label_data_new.withColumn(col, F.col(col).cast('string'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ba68c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "def save_pipeline_model():\n",
    "    \"\"\"\n",
    "    Save the pipeline model to the specified path.\n",
    "    \"\"\"\n",
    "    string_indexers = [E.StringIndexer(inputCol=col, outputCol=col + \"_index\", handleInvalid='keep') for col in categorical_features]\n",
    "    one_hot_encoders = [E.OneHotEncoder(inputCols=[col + \"_index\"], outputCols=[col + \"_ohe\"], handleInvalid='keep') for col in categorical_features]\n",
    "    tokenizers = [E.Tokenizer(inputCol=col, outputCol=col + \"_token\") for col in text_features]\n",
    "    hashing_tfs = [E.HashingTF(inputCol=col + \"_token\", outputCol=col + \"_hash\", numFeatures=16) for col in text_features]\n",
    "    idfs = [E.IDF(inputCol=col + \"_hash\", outputCol=col + \"_tfidf\") for col in text_features]\n",
    "    median_imputer = E.Imputer(inputCols=['new_age', 'active_duration', 'avg_price'], outputCols=['new_age', 'active_duration', 'avg_price'], strategy=\"median\", missingValue=0)\n",
    "    \n",
    "    vector_assembler = E.VectorAssembler(\n",
    "        inputCols=numerical_features + [col + \"_ohe\" for col in categorical_features] + [col + \"_tfidf\" for col in text_features],\n",
    "        outputCol=\"features\",\n",
    "        handleInvalid='keep'\n",
    "    )\n",
    "    pipeline = Pipeline(stages=string_indexers + one_hot_encoders + tokenizers + hashing_tfs + idfs + [median_imputer, vector_assembler])\n",
    "    pipeline_model = pipeline.fit(label_data_new)\n",
    "    return pipeline_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdbcf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = save_pipeline_model()\n",
    "processed_data = pipeline_model.transform(label_data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存预处理管道\n",
    "pipeline_model.write().overwrite().save(\"model/pipeline_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据转换为LightGBM可用格式\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# 选择目标列和特征列\n",
    "df_features = processed_data.select(\"regged\", \"features\")\n",
    "\n",
    "# 转换为稀疏矩阵\n",
    "data, indices, indptr = [], [], [0]\n",
    "for row in df_features.collect():\n",
    "    vec = row.features\n",
    "    data.extend(vec.values)\n",
    "    indices.extend(vec.indices)\n",
    "    indptr.append(len(data))\n",
    "\n",
    "X_sparse = csr_matrix((data, indices, indptr), shape=(df_features.count(), vec.size))\n",
    "y = np.array(df_features.select(\"regged\").rdd.flatMap(lambda x: x).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightgbm_training",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle\n",
    "\n",
    "# LightGBM参数网格\n",
    "param_grid = {\n",
    "    'max_depth': [4, 5, 6],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# 计算类别权重\n",
    "scale_pos_weight = int(pd.Series(y).value_counts()[0] / pd.Series(y).value_counts()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grid_search_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_cv_lgb(param_grid, nfolds):\n",
    "    best_auc = 0\n",
    "    best_params = None\n",
    "    cv_results_all = []\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=nfolds, shuffle=True, random_state=42)\n",
    "\n",
    "    for max_depth, learning_rate, subsample, colsample_bytree in itertools.product(\n",
    "            param_grid['max_depth'], param_grid['learning_rate'],\n",
    "            param_grid['subsample'], param_grid['colsample_bytree']):\n",
    "\n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'scale_pos_weight': scale_pos_weight,\n",
    "            'max_depth': max_depth,\n",
    "            'learning_rate': learning_rate,\n",
    "            'subsample': subsample,\n",
    "            'colsample_bytree': colsample_bytree,\n",
    "            'random_state': 42,\n",
    "            'verbose': -1\n",
    "        }\n",
    "        \n",
    "        fold_aucs = []\n",
    "        \n",
    "        for train_idx, val_idx in skf.split(X_sparse, y):\n",
    "            X_train, X_val = X_sparse[train_idx], X_sparse[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            train_data = lgb.Dataset(X_train, label=y_train)\n",
    "            val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "            \n",
    "            model = lgb.train(\n",
    "                params,\n",
    "                train_data,\n",
    "                valid_sets=[val_data],\n",
    "                num_boost_round=1000,\n",
    "                callbacks=[lgb.early_stopping(20), lgb.log_evaluation(0)]\n",
    "            )\n",
    "            \n",
    "            y_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "            auc = roc_auc_score(y_val, y_pred)\n",
    "            fold_aucs.append(auc)\n",
    "        \n",
    "        mean_auc = np.mean(fold_aucs)\n",
    "        cv_results_all.append({\n",
    "            'params': params,\n",
    "            'val_auc': mean_auc\n",
    "        })\n",
    "        \n",
    "        print(f\"参数: {params}, 平均AUC: {mean_auc:.4f}\")\n",
    "        \n",
    "        if mean_auc > best_auc:\n",
    "            best_auc = mean_auc\n",
    "            best_params = params\n",
    "\n",
    "    print(\"最优参数：\", best_params)\n",
    "    print(\"最优AUC：\", best_auc)\n",
    "    \n",
    "    return best_params, cv_results_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_grid_search",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params, results = grid_search_cv_lgb(param_grid, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_best_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用最优参数训练最终模型\n",
    "train_data = lgb.Dataset(X_sparse, label=y)\n",
    "best_model = lgb.train(\n",
    "    best_params,\n",
    "    train_data,\n",
    "    num_boost_round=1000,\n",
    "    callbacks=[lgb.log_evaluation(100)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "with open('model/best_lgb_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "print(\"模型已保存到 model/best_lgb_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 特征重要性分析\n",
    "feature_importance = best_model.feature_importance(importance_type='gain')\n",
    "feature_names = [f'feature_{i}' for i in range(len(feature_importance))]\n",
    "\n",
    "# 获取前20个最重要的特征\n",
    "top_indices = np.argsort(feature_importance)[-20:]\n",
    "top_importance = feature_importance[top_indices]\n",
    "top_names = [feature_names[i] for i in top_indices]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(top_importance)), top_importance)\n",
    "plt.yticks(range(len(top_importance)), top_names)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 20 Feature Importance (LightGBM)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parameter_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数效果分析\n",
    "results_df = pd.DataFrame(results)\n",
    "param_names = ['max_depth', 'learning_rate', 'subsample', 'colsample_bytree']\n",
    "\n",
    "# 分别绘制val_auc关于每个参数的变化关系\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, param in enumerate(param_names):\n",
    "    grouped = results_df.groupby(results_df['params'].apply(lambda x: x[param]))['val_auc'].mean()\n",
    "    axes[i].plot(grouped.index, grouped.values, marker='o')\n",
    "    axes[i].set_xlabel(param)\n",
    "    axes[i].set_ylabel('val_auc')\n",
    "    axes[i].set_title(f'val_auc vs {param}')\n",
    "    axes[i].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
