{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6965a94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3b87659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 读取本地 cmn.txt 文件内容\n",
    "path = Path(\"C:/Users/DT-Liuxiangfei/Documents/TestCodes/NLPDemo/nmt_demo.ipynb\").parent\n",
    "text = (Path(path).with_name(\"NLPDemo\") / \"cmn.txt\").read_text(encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65bd1cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs=[line.split(\"\\t\") for line in text.splitlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4566249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(pairs)\n",
    "sentences_en, sentences_cmn = zip(*[pair[:2] for pair in pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14c46a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have finished my homework. => 我已經完成我的作業。\n",
      "I'm afraid that you'll have to go in person. => 我恐怕您必须亲自去。\n",
      "Are you allergic to any medicine? => 你對任何藥物過敏嗎？\n",
      "You're really a hard worker. => 你真是个努力的工人。\n",
      "I often played tennis with her. => 我常常和她打網球。\n",
      "Tom poured cold water over himself to wake himself up. => 汤姆给自己浇凉水以清醒过来。\n",
      "The war ended in 1945. => 这场战争结束于1945年。\n",
      "I bought a red sports car. => 我買了一輛紅色的跑車。\n",
      "Should I buy a wireless mouse? => 要不要買個無線鼠標呢？\n",
      "She dusts the furniture every day. => 她每天擦拭家具的灰塵。\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"{sentences_en[i]} => {sentences_cmn[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "febe338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "max_length = 50\n",
    "text_vec_layer_en = tf.keras.layers.TextVectorization(vocab_size, output_sequence_length=max_length)\n",
    "text_vec_layer_cmn = tf.keras.layers.TextVectorization(vocab_size, output_sequence_length=max_length)\n",
    "text_vec_layer_en.adapt(sentences_en)\n",
    "text_vec_layer_cmn.adapt([f\"startofseq {s} endofseq\" for s in sentences_cmn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250b1b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['',\n",
       "  '[UNK]',\n",
       "  np.str_('the'),\n",
       "  np.str_('i'),\n",
       "  np.str_('to'),\n",
       "  np.str_('you'),\n",
       "  np.str_('a'),\n",
       "  np.str_('is'),\n",
       "  np.str_('tom'),\n",
       "  np.str_('he')],\n",
       " ['',\n",
       "  '[UNK]',\n",
       "  np.str_('startofseq'),\n",
       "  np.str_('endofseq'),\n",
       "  np.str_('多少錢？'),\n",
       "  np.str_('请稍等一下。'),\n",
       "  np.str_('每个人都知道汤姆的法语很好。'),\n",
       "  np.str_('我有点累了。'),\n",
       "  np.str_('聖誕節快到了。'),\n",
       "  np.str_('我也是這麼想的。')])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer_en.get_vocabulary()[:10]\n",
    "text_vec_layer_cmn.get_vocabulary()[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
