{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42f28cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "474afddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['phone_model', 'browser_family', 'os_family', 'device_brand', 'city_code', 'province_code',\n",
    "                        'sex', 'hashouse', 'social', 'overdue',\n",
    "                        'tax', 'married', 'benke', 'kid', 'income', 'consumption', 'shebao']\n",
    "\n",
    "text_features = ['clicked_products_0009', 'clicked_products_date_0009', 'sms_sent_products_0009',\n",
    "                 'sms_sent_products_1019', 'sms_sent_products_date_0009', 'sms_sent_products_date_1019',\n",
    "                 'called_products_0009', 'called_products_1019', 'called_products_date_0009',\n",
    "                 'called_products_date_1019', 'picked_products_0009', 'picked_products_date_0009',\n",
    "                 'outbound_sent_products_0009', 'outbound_sent_products_date_0009', 'set_all_ins_host_180', 'set_all_ins_host_360',] \\\n",
    "                + ['keypress_30', 'keypress_60', 'keypress_90', 'keypress_120',\n",
    "                   'rule_name_30', 'rule_name_60', 'rule_name_90', 'rule_name_120',\n",
    "                   'semantic_30', 'semantic_60', 'semantic_90', 'semantic_120', 'model_value']\n",
    "\n",
    "text_feature_types = ['products', 'keypress', 'rules', 'semantics', 'insurances', 'model_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e28fe500",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_excel(\"../../data/train_data_ifh4.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb51ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[text_features] = train_df[text_features].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a1ce86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phone_model</th>\n",
       "      <th>browser_family</th>\n",
       "      <th>os_family</th>\n",
       "      <th>device_brand</th>\n",
       "      <th>city_code</th>\n",
       "      <th>province_code</th>\n",
       "      <th>sex</th>\n",
       "      <th>hashouse</th>\n",
       "      <th>social</th>\n",
       "      <th>overdue</th>\n",
       "      <th>tax</th>\n",
       "      <th>married</th>\n",
       "      <th>benke</th>\n",
       "      <th>kid</th>\n",
       "      <th>income</th>\n",
       "      <th>consumption</th>\n",
       "      <th>shebao</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   phone_model  browser_family  os_family  device_brand  city_code  \\\n",
       "0            2               0          1             0         24   \n",
       "1            2               1          0             1         28   \n",
       "2            3               0          1             0         15   \n",
       "3           10               1          0             1        103   \n",
       "4            5               0          1             0          1   \n",
       "\n",
       "   province_code  sex  hashouse  social  overdue  tax  married  benke  kid  \\\n",
       "0              5    2         0       1        0    0        0      0    0   \n",
       "1              2    1         0       0        0    0        0      0    0   \n",
       "2              3    1         0       0        0    0        0      1    0   \n",
       "3              5    1         0       0        0    0        0      1    0   \n",
       "4              0    2         0       0        0    0        0      0    0   \n",
       "\n",
       "   income  consumption  shebao  \n",
       "0       0            0       0  \n",
       "1       0            0       0  \n",
       "2       0            0       0  \n",
       "3       0            0       0  \n",
       "4       0            0       0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[categorical_features].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa76d9bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clicked_products_0009</th>\n",
       "      <th>clicked_products_date_0009</th>\n",
       "      <th>sms_sent_products_0009</th>\n",
       "      <th>sms_sent_products_1019</th>\n",
       "      <th>sms_sent_products_date_0009</th>\n",
       "      <th>sms_sent_products_date_1019</th>\n",
       "      <th>called_products_0009</th>\n",
       "      <th>called_products_1019</th>\n",
       "      <th>called_products_date_0009</th>\n",
       "      <th>called_products_date_1019</th>\n",
       "      <th>...</th>\n",
       "      <th>keypress_120</th>\n",
       "      <th>rule_name_30</th>\n",
       "      <th>rule_name_60</th>\n",
       "      <th>rule_name_90</th>\n",
       "      <th>rule_name_120</th>\n",
       "      <th>semantic_30</th>\n",
       "      <th>semantic_60</th>\n",
       "      <th>semantic_90</th>\n",
       "      <th>semantic_120</th>\n",
       "      <th>model_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>...</td>\n",
       "      <td>小助理 运营商提示音</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>baotai27_其他</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>144 145 140 143</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>IYBPAZX_ZTKMF_OPPOJX_BT IYBPAZX_ZTKMF_OPPOJX_BT</td>\n",
       "      <td>nan</td>\n",
       "      <td>2D 4D</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>baotai27_G</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>7 9 140 150 151 7 131 131</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FQL IYBPAZX_ZMF_CS_BT NWZAZX_ZNWZAMF_NEWOPPO_B...</td>\n",
       "      <td>12D 12D 15D 24D 30D 32D 36D 38D 41D 44D</td>\n",
       "      <td>IYBPAZX_ZTKMF_OPPOJX_BT NWZAZX_ZNWZAMF_NEWOPPO...</td>\n",
       "      <td>IYBPAZX_ZTKMF_OPPOJX_BT IYBPAZX_ZMF_CS_BT IYBP...</td>\n",
       "      <td>2D 4D 8D 10D 12D 15D 20D 24D 26D 28D</td>\n",
       "      <td>30D 32D 34D 36D 39D 43D 45D 47D 51D 53D</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>...</td>\n",
       "      <td>触发发短信 sendMessage_special 没输入手机号 输入手机号2 输入手机号3...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>yingdian888_B jiyonghua661_B huirong888_D ying...</td>\n",
       "      <td>mayi02_D baotai14_B mayi02_D mayi02_D</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>2240 2572 2672 2313 2345 2672 2313 2313 2313 2...</td>\n",
       "      <td>23 24 46 3921 35 21 21 2576 2181 2189 24 23 23...</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>IYBPAZX_ZTKMF_OPPOJX_BT IYBPAZX_ZMF_CS_BT IYBP...</td>\n",
       "      <td>nan</td>\n",
       "      <td>2D 4D 6D</td>\n",
       "      <td>nan</td>\n",
       "      <td>IYBPAZX_TKMF_GD_BZ_GZH_WH IYBPAZX_TKMF_GD_BZ_G...</td>\n",
       "      <td>nan</td>\n",
       "      <td>1D 4D 7D 52D 55D 58D</td>\n",
       "      <td>nan</td>\n",
       "      <td>...</td>\n",
       "      <td>触发发短信 sendMessage_special</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>baotai27_D</td>\n",
       "      <td>baotai27_其他</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               clicked_products_0009  \\\n",
       "0                                                nan   \n",
       "1                                                nan   \n",
       "2                                                nan   \n",
       "3  FQL IYBPAZX_ZMF_CS_BT NWZAZX_ZNWZAMF_NEWOPPO_B...   \n",
       "4                                                nan   \n",
       "\n",
       "                clicked_products_date_0009  \\\n",
       "0                                      nan   \n",
       "1                                      nan   \n",
       "2                                      nan   \n",
       "3  12D 12D 15D 24D 30D 32D 36D 38D 41D 44D   \n",
       "4                                      nan   \n",
       "\n",
       "                              sms_sent_products_0009  \\\n",
       "0                                                nan   \n",
       "1    IYBPAZX_ZTKMF_OPPOJX_BT IYBPAZX_ZTKMF_OPPOJX_BT   \n",
       "2                                                nan   \n",
       "3  IYBPAZX_ZTKMF_OPPOJX_BT NWZAZX_ZNWZAMF_NEWOPPO...   \n",
       "4  IYBPAZX_ZTKMF_OPPOJX_BT IYBPAZX_ZMF_CS_BT IYBP...   \n",
       "\n",
       "                              sms_sent_products_1019  \\\n",
       "0                                                nan   \n",
       "1                                                nan   \n",
       "2                                                nan   \n",
       "3  IYBPAZX_ZTKMF_OPPOJX_BT IYBPAZX_ZMF_CS_BT IYBP...   \n",
       "4                                                nan   \n",
       "\n",
       "            sms_sent_products_date_0009  \\\n",
       "0                                   nan   \n",
       "1                                 2D 4D   \n",
       "2                                   nan   \n",
       "3  2D 4D 8D 10D 12D 15D 20D 24D 26D 28D   \n",
       "4                              2D 4D 6D   \n",
       "\n",
       "               sms_sent_products_date_1019  \\\n",
       "0                                      nan   \n",
       "1                                      nan   \n",
       "2                                      nan   \n",
       "3  30D 32D 34D 36D 39D 43D 45D 47D 51D 53D   \n",
       "4                                      nan   \n",
       "\n",
       "                                called_products_0009 called_products_1019  \\\n",
       "0                                                nan                  nan   \n",
       "1                                                nan                  nan   \n",
       "2                                                nan                  nan   \n",
       "3                                                nan                  nan   \n",
       "4  IYBPAZX_TKMF_GD_BZ_GZH_WH IYBPAZX_TKMF_GD_BZ_G...                  nan   \n",
       "\n",
       "  called_products_date_0009 called_products_date_1019  ...  \\\n",
       "0                       nan                       nan  ...   \n",
       "1                       nan                       nan  ...   \n",
       "2                       nan                       nan  ...   \n",
       "3                       nan                       nan  ...   \n",
       "4      1D 4D 7D 52D 55D 58D                       nan  ...   \n",
       "\n",
       "                                        keypress_120 rule_name_30  \\\n",
       "0                                         小助理 运营商提示音          nan   \n",
       "1                                                nan          nan   \n",
       "2                                                nan          nan   \n",
       "3  触发发短信 sendMessage_special 没输入手机号 输入手机号2 输入手机号3...          nan   \n",
       "4                          触发发短信 sendMessage_special          nan   \n",
       "\n",
       "  rule_name_60                                       rule_name_90  \\\n",
       "0          nan                                                nan   \n",
       "1          nan                                                nan   \n",
       "2          nan                                         baotai27_G   \n",
       "3          nan  yingdian888_B jiyonghua661_B huirong888_D ying...   \n",
       "4          nan                                         baotai27_D   \n",
       "\n",
       "                           rule_name_120 semantic_30 semantic_60  \\\n",
       "0                            baotai27_其他         nan         nan   \n",
       "1                                    nan         nan         nan   \n",
       "2                                    nan         nan         nan   \n",
       "3  mayi02_D baotai14_B mayi02_D mayi02_D         nan         nan   \n",
       "4                            baotai27_其他         nan         nan   \n",
       "\n",
       "                                         semantic_90  \\\n",
       "0                                                nan   \n",
       "1                                                nan   \n",
       "2                          7 9 140 150 151 7 131 131   \n",
       "3  2240 2572 2672 2313 2345 2672 2313 2313 2313 2...   \n",
       "4                                                nan   \n",
       "\n",
       "                                        semantic_120 model_value  \n",
       "0                                    144 145 140 143         nan  \n",
       "1                                                nan         nan  \n",
       "2                                                nan         nan  \n",
       "3  23 24 46 3921 35 21 21 2576 2181 2189 24 23 23...         nan  \n",
       "4                                                nan         nan  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[text_features].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1b3a5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(feature_type):\n",
    "    words = train_df[feature_type].dropna().str.split(' ')\n",
    "    exploded_words = words.explode()\n",
    "    vocabulary = exploded_words.value_counts()\n",
    "    vocabulary = vocabulary[vocabulary > 5]\n",
    "    vocab_size = vocabulary.shape[0]\n",
    "    vocabulary = vocabulary.index.tolist()\n",
    "\n",
    "    return vocabulary, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b542d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {}\n",
    "for type in text_feature_types:\n",
    "    vocab_dict[type] = get_vocabulary(type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99e7e417",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_dict = {col: vocab_info[1]+100 for col, vocab_info in vocab_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9818728c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'products': 273,\n",
       " 'keypress': 207,\n",
       " 'rules': 328,\n",
       " 'semantics': 351,\n",
       " 'insurances': 141,\n",
       " 'model_value': 102}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868d7ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only first time\n",
    "import pickle\n",
    "\n",
    "\n",
    "max_processes = 5\n",
    "oov_tok = '<OOV>'\n",
    "\n",
    "\n",
    "def get_token(text_feature_type):\n",
    "    print(\"fitting tokenizer:\", text_feature_type)\n",
    "    token = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size_dict[text_feature_type],\n",
    "                                                  oov_token=oov_tok, filters='')\n",
    "    token.fit_on_texts(train_df[text_feature_type].fillna(\"\").astype(str))\n",
    "    return {text_feature_type: token}\n",
    "\n",
    "\n",
    "result_list = [get_token(t) for t in text_feature_types]\n",
    "result_dict = {}\n",
    "for d in result_list:\n",
    "    result_dict.update(d)\n",
    "\n",
    "token_dict = {}\n",
    "\n",
    "for text_feature_type in text_feature_types:\n",
    "    token_dict[text_feature_type] = result_dict[text_feature_type]\n",
    "\n",
    "with open(f'../../data/dicts/token_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(token_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd701547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'../../data/dicts/token_dict.pkl', 'rb') as f:\n",
    "    token_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e60349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_dict = {\n",
    "    'called_products_0009': 10,\n",
    "    'called_products_1019': 6,\n",
    "    'called_products_date_0009': 10,\n",
    "    'called_products_date_1019': 6,\n",
    "    'clicked_products_0009': 3,\n",
    "    'clicked_products_date_0009': 3,\n",
    "    'keypress_120': 3,\n",
    "    'keypress_30': 3,\n",
    "    'keypress_60': 3,\n",
    "    'keypress_90': 3,\n",
    "    'label_0009': 3,\n",
    "    'label_1019': 3,\n",
    "    'label_date_0009': 3,\n",
    "    'label_date_1019': 3,\n",
    "    'model_value': 3,\n",
    "    'outbound_sent_products_0009': 3,\n",
    "    'outbound_sent_products_date_0009': 3,\n",
    "    'picked_products_0009': 3,\n",
    "    'picked_products_date_0009': 3,\n",
    "    'rule_name_120': 3,\n",
    "    'rule_name_30': 3,\n",
    "    'rule_name_60': 3,\n",
    "    'rule_name_90': 3,\n",
    "    'semantic_120': 5,\n",
    "    'semantic_30': 3,\n",
    "    'semantic_60': 3,\n",
    "    'semantic_90': 5,\n",
    "    'set_all_ins_host_180': 5,\n",
    "    'set_all_ins_host_360': 4,\n",
    "    'sms_sent_products_0009': 10,\n",
    "    'sms_sent_products_1019': 9,\n",
    "    'sms_sent_products_date_0009': 10,\n",
    "    'sms_sent_products_date_1019': 9\n",
    " }\n",
    "\n",
    "category_counts_dict = {col: train_df[col].nunique() for col in categorical_features}\n",
    "\n",
    "\n",
    "def get_text_feature_type(col):\n",
    "    if 'set_' in col:\n",
    "        text_feature_type = 'insurances'\n",
    "    elif 'host_' in col:\n",
    "        text_feature_type = 'hosts'\n",
    "    elif 'products_' in col:\n",
    "        text_feature_type = 'products'\n",
    "    elif 'label_' in col:\n",
    "        text_feature_type = 'labels'\n",
    "    elif 'keypress_' in col:\n",
    "        text_feature_type = 'keypress'\n",
    "    elif 'rule_name_' in col:\n",
    "        text_feature_type = 'rules'\n",
    "    elif 'semantic_' in col:\n",
    "        text_feature_type = 'semantics'\n",
    "    else:\n",
    "        text_feature_type = 'model_value'\n",
    "    return text_feature_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419c2c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tokenize : clicked_products_0009\n",
      " tokenize : clicked_products_date_0009\n",
      " tokenize : sms_sent_products_0009\n",
      " tokenize : sms_sent_products_1019\n",
      " tokenize : sms_sent_products_date_0009\n",
      " tokenize : sms_sent_products_date_1019\n",
      " tokenize : called_products_0009\n",
      " tokenize : called_products_1019\n",
      " tokenize : called_products_date_0009\n",
      " tokenize : called_products_date_1019\n",
      " tokenize : picked_products_0009\n",
      " tokenize : picked_products_date_0009\n",
      " tokenize : outbound_sent_products_0009\n",
      " tokenize : outbound_sent_products_date_0009\n",
      " tokenize : set_all_ins_host_180\n",
      " tokenize : set_all_ins_host_360\n",
      " tokenize : keypress_30\n",
      " tokenize : keypress_60\n",
      " tokenize : keypress_90\n",
      " tokenize : keypress_120\n",
      " tokenize : rule_name_30\n",
      " tokenize : rule_name_60\n",
      " tokenize : rule_name_90\n",
      " tokenize : rule_name_120\n",
      " tokenize : semantic_30\n",
      " tokenize : semantic_60\n",
      " tokenize : semantic_90\n",
      " tokenize : semantic_120\n",
      " tokenize : model_value\n"
     ]
    }
   ],
   "source": [
    "# tokenization and padding\n",
    "padding_type = 'post'\n",
    "truncate_type = 'post'\n",
    "\n",
    "\n",
    "def tokenize(col):\n",
    "    print(\" tokenize :\", col)\n",
    "    text_feature_type = get_text_feature_type(col)\n",
    "    token = token_dict[text_feature_type]\n",
    "    max_len = max_len_dict[col]\n",
    "    tokenized_seq = token.texts_to_sequences(train_df[col])\n",
    "    result_train = tf.keras.preprocessing.sequence.pad_sequences(tokenized_seq, maxlen=max_len, padding=padding_type,\n",
    "                                                                 truncating=truncate_type)\n",
    "    with open(f'../../data/text_features/{col}.pkl', 'wb') as f:\n",
    "        pickle.dump(result_train, f)\n",
    "\n",
    "for col in text_features:\n",
    "    tokenize(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77afc4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore numerical features\n",
    "restored_raw = (\n",
    "    train_df[\"numerical_features\"]\n",
    "    .astype(str)\n",
    "    .str.replace(\"\\n\", \" \", regex=False)\n",
    "    .str.strip(\"[]\")\n",
    "    .str.strip()\n",
    "    .str.split(r\"\\s+\", expand=True)\n",
    "    .apply(lambda col: pd.to_numeric(col.replace('', pd.NA), errors='coerce'))\n",
    "    .fillna(0)\n",
    "    .to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "131b7ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_train_cro = train_df['label'].str.strip(\"[]\").str.split(\" \", expand=True).astype(int).to_numpy()\n",
    "\n",
    "# X\n",
    "X_train = {'numerical_features': restored_raw}\n",
    "\n",
    "for col in categorical_features:\n",
    "    X_train[col] = np.array(train_df[col])\n",
    "\n",
    "for col in text_features:\n",
    "    with open(f'../../data/text_features/{col}.pkl', 'rb') as f:\n",
    "        X_train[col] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6bb553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 拆分索引\n",
    "indices = np.arange(len(y_train_cro))\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42, stratify=y_train_cro)\n",
    "\n",
    "# 拆分 y\n",
    "y_train_split = y_train_cro[train_idx]\n",
    "y_test_split = y_train_cro[test_idx]\n",
    "\n",
    "# 拆分 X\n",
    "X_train_split = {}\n",
    "X_test_split = {}\n",
    "for key in X_train:\n",
    "    arr = X_train[key]\n",
    "    X_train_split[key] = arr[train_idx]\n",
    "    X_test_split[key] = arr[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4fb5821",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wide_layer(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.b = self.add_weight(name='b', shape=(1,), initializer='zeros', trainable=True)\n",
    "        self.w = self.add_weight(name='w', shape=(input_shape[-1], 1), initializer='glorot_normal', trainable=True, regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "\n",
    "    def call(self, inputs, **kwargs):   #输入为 dense_inputs\n",
    "        x = tf.matmul(inputs, self.w) + self.b     #shape: (batchsize, 1)\n",
    "        return x\n",
    "\n",
    "class Deep_layer(Layer):\n",
    "    def __init__(self, hidden_units, output_dim, activation):\n",
    "        super().__init__()\n",
    "        self.hidden_layer = [Dense(i, activation=activation) for i in hidden_units]\n",
    "        self.output_layer = Dense(output_dim, activation=None)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = inputs\n",
    "        for layer in self.hidden_layer:\n",
    "            x = layer(x)\n",
    "        output = self.output_layer(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0a68bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def wide_deep_model(output_unit, l2_reg_text_embedding=0.0, l2_reg_categorical_embedding=0.0):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # text inputs\n",
    "    text_inputs = []\n",
    "    for col in text_features:\n",
    "        text_inputs.append(tf.keras.layers.Input(shape=(max_len_dict[col],), name=col))\n",
    "    text_embeddings = []\n",
    "    for i, col in enumerate(text_features):\n",
    "        text_feature_type = get_text_feature_type(col)\n",
    "        text_embeddings.append(\n",
    "            tf.keras.layers.Embedding(\n",
    "                vocab_size_dict[text_feature_type]+1, \n",
    "                int(np.log1p(vocab_size_dict[text_feature_type])+2),\n",
    "                embeddings_regularizer=tf.keras.regularizers.l2(l2_reg_text_embedding), \n",
    "                name=col + '_embed')(text_inputs[i])\n",
    "        )\n",
    "\n",
    "    text_logit = tf.keras.layers.Concatenate(name='text_concat')(\n",
    "        [tf.keras.layers.GlobalAveragePooling1D()(text_emb) for text_emb in text_embeddings]\n",
    "    )\n",
    "\n",
    "    # categorical inputs\n",
    "    # one-hot inputs for categorical features (as Keras Input layers + one-hot encoding layers)\n",
    "    categorical_inputs = []\n",
    "    onehot_inputs = []\n",
    "    onehot_vectors = []\n",
    "    for col in categorical_features:\n",
    "        inp = tf.keras.layers.Input(shape=(1,), name=col)\n",
    "        categorical_inputs.append(inp)\n",
    "        onehot_inputs.append(inp)\n",
    "        # CategoryEncoding expects integer indices; output_mode='one_hot' produces a vector of length `vocab`\n",
    "        one_hot_vec = tf.keras.layers.CategoryEncoding(num_tokens=category_counts_dict[col] + 2, output_mode='one_hot', name=col + '_one_hot')(inp)\n",
    "        onehot_vectors.append(tf.keras.layers.Flatten()(one_hot_vec))\n",
    "\n",
    "    # feature crossing: use hashing to keep cross dimension bounded\n",
    "    cross_hash_bins = 128\n",
    "    crossed_vectors = []\n",
    "    # limit crosses if you want — here we use all pairwise crosses; comment/change if too many\n",
    "    for a, b in combinations(categorical_features, 2):\n",
    "        # get the corresponding input tensors by name\n",
    "        inp_a = next(x for x in onehot_inputs if x.name.startswith(a))\n",
    "        inp_b = next(x for x in onehot_inputs if x.name.startswith(b))\n",
    "        # build a combined string token for the pair and hash it to an index\n",
    "        pair_str = tf.keras.layers.Lambda(lambda x: tf.strings.join([tf.strings.as_string(x[0]), '_', tf.strings.as_string(x[1])]))([inp_a, inp_b])\n",
    "        hashed_idx = tf.keras.layers.Hashing(num_bins=cross_hash_bins, name=f'hash_{a}_{b}')(pair_str)\n",
    "        cross_one_hot = tf.keras.layers.CategoryEncoding(num_tokens=cross_hash_bins, output_mode='one_hot', name=f'cross_oh_{a}_{b}')(hashed_idx)\n",
    "        crossed_vectors.append(tf.keras.layers.Flatten()(cross_one_hot))\n",
    "\n",
    "    # final one-hot / cross concatenation (will be available as tensor `onehot_concat` for downstream use)\n",
    "    onehot_concat = tf.keras.layers.Concatenate(name='onehot_concat')(onehot_vectors + crossed_vectors)\n",
    "\n",
    "    categorical_embeddings = []\n",
    "    for i, col in enumerate(categorical_features):\n",
    "        categorical_embeddings.append(\n",
    "            tf.keras.layers.Embedding(category_counts_dict[col]+2, int(np.log1p(category_counts_dict[col]) + 1),\n",
    "                                      embeddings_regularizer=tf.keras.regularizers.l2(l2_reg_categorical_embedding),\n",
    "                                      name=col + '_embed')(categorical_inputs[i])\n",
    "        )\n",
    "\n",
    "    categorical_logit = tf.keras.layers.Concatenate(name='categorical_concat')(\n",
    "        [tf.keras.layers.Flatten()(cat_emb) for cat_emb in categorical_embeddings]\n",
    "    )\n",
    "\n",
    "    # numerical inputs\n",
    "    numerical_input = tf.keras.layers.Input(shape=(230,), name='numerical_features')\n",
    "\n",
    "    # wide部分\n",
    "    wide_input = tf.keras.layers.Concatenate(name='wide_concat')([numerical_input, onehot_concat])\n",
    "    wide_output = Wide_layer()(wide_input)\n",
    "\n",
    "    # deep部分\n",
    "    deep_input = tf.keras.layers.Concatenate(name='deep_concat')([numerical_input, text_logit, categorical_logit])\n",
    "    deep_output = Deep_layer(hidden_units=[256, 128, 64], output_dim=8, activation='relu')(deep_input)\n",
    "\n",
    "    # output\n",
    "    outputs = tf.keras.layers.Dense(output_unit, activation='sigmoid')(tf.keras.layers.Concatenate()([wide_output, deep_output]))\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=text_inputs + categorical_inputs + [numerical_input], outputs=outputs, name='base_model')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cc2dbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "n_epochs = 100\n",
    "batch_size = 1024\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.4, patience=1, min_lr=0.00005, verbose=1)\n",
    "callbacks = [es, reduce_lr]\n",
    "\n",
    "\n",
    "def train_model(x_train, y_train, x_test, y_test):\n",
    "    output_units = 3\n",
    "    model = wide_deep_model(output_units)\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy(name='acc'), tf.keras.metrics.AUC(name='auc')],\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr))\n",
    "    \n",
    "    model.fit(x_train, y_train, validation_data=(x_test, y_test),\n",
    "              epochs=n_epochs, batch_size=batch_size, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7f28d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DT-Liuxiangfei\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "Epoch 1/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - acc: 0.9030 - auc: 0.9276 - loss: 0.5325 - val_acc: 0.9347 - val_auc: 0.9413 - val_loss: 0.4809 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 107ms/step - acc: 0.9428 - auc: 0.9446 - loss: 0.4526 - val_acc: 0.9491 - val_auc: 0.9457 - val_loss: 0.4113 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - acc: 0.9514 - auc: 0.9473 - loss: 0.3875 - val_acc: 0.9531 - val_auc: 0.9466 - val_loss: 0.3541 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - acc: 0.9537 - auc: 0.9477 - loss: 0.3341 - val_acc: 0.9541 - val_auc: 0.9468 - val_loss: 0.3061 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - acc: 0.9540 - auc: 0.9486 - loss: 0.2887 - val_acc: 0.9541 - val_auc: 0.9472 - val_loss: 0.2666 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - acc: 0.9540 - auc: 0.9493 - loss: 0.2521 - val_acc: 0.9541 - val_auc: 0.9478 - val_loss: 0.2360 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - acc: 0.9540 - auc: 0.9507 - loss: 0.2242 - val_acc: 0.9541 - val_auc: 0.9487 - val_loss: 0.2143 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - acc: 0.9540 - auc: 0.9524 - loss: 0.2046 - val_acc: 0.9541 - val_auc: 0.9498 - val_loss: 0.1996 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - acc: 0.9540 - auc: 0.9541 - loss: 0.1914 - val_acc: 0.9541 - val_auc: 0.9509 - val_loss: 0.1898 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - acc: 0.9540 - auc: 0.9558 - loss: 0.1823 - val_acc: 0.9541 - val_auc: 0.9518 - val_loss: 0.1833 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - acc: 0.9540 - auc: 0.9573 - loss: 0.1760 - val_acc: 0.9541 - val_auc: 0.9527 - val_loss: 0.1787 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - acc: 0.9540 - auc: 0.9587 - loss: 0.1713 - val_acc: 0.9541 - val_auc: 0.9536 - val_loss: 0.1754 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - acc: 0.9540 - auc: 0.9601 - loss: 0.1677 - val_acc: 0.9541 - val_auc: 0.9544 - val_loss: 0.1729 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - acc: 0.9540 - auc: 0.9612 - loss: 0.1649 - val_acc: 0.9541 - val_auc: 0.9551 - val_loss: 0.1708 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - acc: 0.9540 - auc: 0.9623 - loss: 0.1625 - val_acc: 0.9541 - val_auc: 0.9558 - val_loss: 0.1692 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - acc: 0.9540 - auc: 0.9633 - loss: 0.1605 - val_acc: 0.9541 - val_auc: 0.9564 - val_loss: 0.1678 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - acc: 0.9541 - auc: 0.9642 - loss: 0.1587 - val_acc: 0.9541 - val_auc: 0.9570 - val_loss: 0.1666 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - acc: 0.9541 - auc: 0.9652 - loss: 0.1571 - val_acc: 0.9540 - val_auc: 0.9576 - val_loss: 0.1656 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step - acc: 0.9541 - auc: 0.9660 - loss: 0.1556 - val_acc: 0.9540 - val_auc: 0.9580 - val_loss: 0.1648 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 113ms/step - acc: 0.9541 - auc: 0.9669 - loss: 0.1542 - val_acc: 0.9540 - val_auc: 0.9586 - val_loss: 0.1641 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step - acc: 0.9541 - auc: 0.9677 - loss: 0.1529 - val_acc: 0.9540 - val_auc: 0.9589 - val_loss: 0.1634 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - acc: 0.9542 - auc: 0.9684 - loss: 0.1518 - val_acc: 0.9540 - val_auc: 0.9594 - val_loss: 0.1629 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - acc: 0.9542 - auc: 0.9691 - loss: 0.1506 - val_acc: 0.9540 - val_auc: 0.9598 - val_loss: 0.1624 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 134ms/step - acc: 0.9542 - auc: 0.9697 - loss: 0.1495 - val_acc: 0.9540 - val_auc: 0.9600 - val_loss: 0.1620 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - acc: 0.9543 - auc: 0.9703 - loss: 0.1484 - val_acc: 0.9540 - val_auc: 0.9602 - val_loss: 0.1616 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - acc: 0.9545 - auc: 0.9710 - loss: 0.1474 - val_acc: 0.9540 - val_auc: 0.9604 - val_loss: 0.1613 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - acc: 0.9545 - auc: 0.9716 - loss: 0.1464 - val_acc: 0.9540 - val_auc: 0.9608 - val_loss: 0.1610 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - acc: 0.9545 - auc: 0.9723 - loss: 0.1455 - val_acc: 0.9540 - val_auc: 0.9610 - val_loss: 0.1608 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - acc: 0.9545 - auc: 0.9728 - loss: 0.1445 - val_acc: 0.9540 - val_auc: 0.9612 - val_loss: 0.1606 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - acc: 0.9560 - auc: 0.9739 - loss: 0.1411\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - acc: 0.9546 - auc: 0.9733 - loss: 0.1437 - val_acc: 0.9540 - val_auc: 0.9613 - val_loss: 0.1605 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - acc: 0.9547 - auc: 0.9737 - loss: 0.1429 - val_acc: 0.9540 - val_auc: 0.9614 - val_loss: 0.1604 - learning_rate: 5.0000e-05\n",
      "Epoch 32/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - acc: 0.9547 - auc: 0.9740 - loss: 0.1424 - val_acc: 0.9540 - val_auc: 0.9614 - val_loss: 0.1604 - learning_rate: 5.0000e-05\n",
      "Epoch 33/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step - acc: 0.9548 - auc: 0.9742 - loss: 0.1420 - val_acc: 0.9540 - val_auc: 0.9615 - val_loss: 0.1603 - learning_rate: 5.0000e-05\n",
      "Epoch 34/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - acc: 0.9548 - auc: 0.9744 - loss: 0.1416 - val_acc: 0.9540 - val_auc: 0.9616 - val_loss: 0.1603 - learning_rate: 5.0000e-05\n",
      "Epoch 35/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step - acc: 0.9549 - auc: 0.9747 - loss: 0.1411 - val_acc: 0.9540 - val_auc: 0.9616 - val_loss: 0.1602 - learning_rate: 5.0000e-05\n",
      "Epoch 36/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - acc: 0.9551 - auc: 0.9749 - loss: 0.1407 - val_acc: 0.9540 - val_auc: 0.9618 - val_loss: 0.1602 - learning_rate: 5.0000e-05\n",
      "Epoch 37/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - acc: 0.9550 - auc: 0.9752 - loss: 0.1403 - val_acc: 0.9540 - val_auc: 0.9618 - val_loss: 0.1602 - learning_rate: 5.0000e-05\n",
      "Epoch 38/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - acc: 0.9550 - auc: 0.9754 - loss: 0.1398 - val_acc: 0.9540 - val_auc: 0.9619 - val_loss: 0.1601 - learning_rate: 5.0000e-05\n",
      "Epoch 39/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - acc: 0.9551 - auc: 0.9756 - loss: 0.1394 - val_acc: 0.9540 - val_auc: 0.9620 - val_loss: 0.1601 - learning_rate: 5.0000e-05\n",
      "Epoch 40/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - acc: 0.9551 - auc: 0.9759 - loss: 0.1390 - val_acc: 0.9540 - val_auc: 0.9619 - val_loss: 0.1601 - learning_rate: 5.0000e-05\n",
      "Epoch 41/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step - acc: 0.9551 - auc: 0.9761 - loss: 0.1386 - val_acc: 0.9540 - val_auc: 0.9620 - val_loss: 0.1601 - learning_rate: 5.0000e-05\n",
      "Epoch 42/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step - acc: 0.9551 - auc: 0.9763 - loss: 0.1381 - val_acc: 0.9540 - val_auc: 0.9620 - val_loss: 0.1601 - learning_rate: 5.0000e-05\n"
     ]
    }
   ],
   "source": [
    "train_model(X_train_split, y_train_split, X_test_split, y_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b036965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c1fa41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
