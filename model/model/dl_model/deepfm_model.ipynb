{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a42f28cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "474afddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['phone_model', 'browser_family', 'os_family', 'device_brand', 'city_code', 'province_code',\n",
    "                        'sex', 'hashouse', 'social', 'overdue',\n",
    "                        'tax', 'married', 'benke', 'kid', 'income', 'consumption', 'shebao']\n",
    "\n",
    "text_features = ['clicked_products_0009', 'clicked_products_date_0009', 'sms_sent_products_0009',\n",
    "                 'sms_sent_products_1019', 'sms_sent_products_date_0009', 'sms_sent_products_date_1019',\n",
    "                 'called_products_0009', 'called_products_1019', 'called_products_date_0009',\n",
    "                 'called_products_date_1019', 'picked_products_0009', 'picked_products_date_0009',\n",
    "                 'outbound_sent_products_0009', 'outbound_sent_products_date_0009', 'set_all_ins_host_180', 'set_all_ins_host_360',] \\\n",
    "                + ['keypress_30', 'keypress_60', 'keypress_90', 'keypress_120',\n",
    "                   'rule_name_30', 'rule_name_60', 'rule_name_90', 'rule_name_120',\n",
    "                   'semantic_30', 'semantic_60', 'semantic_90', 'semantic_120', 'model_value']\n",
    "\n",
    "text_feature_types = ['products', 'keypress', 'rules', 'semantics', 'insurances', 'model_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e28fe500",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_excel(\"../../data/train_data_ifh4.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb51ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[text_features] = train_df[text_features].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a1ce86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phone_model</th>\n",
       "      <th>browser_family</th>\n",
       "      <th>os_family</th>\n",
       "      <th>device_brand</th>\n",
       "      <th>city_code</th>\n",
       "      <th>province_code</th>\n",
       "      <th>sex</th>\n",
       "      <th>hashouse</th>\n",
       "      <th>social</th>\n",
       "      <th>overdue</th>\n",
       "      <th>tax</th>\n",
       "      <th>married</th>\n",
       "      <th>benke</th>\n",
       "      <th>kid</th>\n",
       "      <th>income</th>\n",
       "      <th>consumption</th>\n",
       "      <th>shebao</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   phone_model  browser_family  os_family  device_brand  city_code  \\\n",
       "0            2               0          1             0         24   \n",
       "1            2               1          0             1         28   \n",
       "2            3               0          1             0         15   \n",
       "3           10               1          0             1        103   \n",
       "4            5               0          1             0          1   \n",
       "\n",
       "   province_code  sex  hashouse  social  overdue  tax  married  benke  kid  \\\n",
       "0              5    2         0       1        0    0        0      0    0   \n",
       "1              2    1         0       0        0    0        0      0    0   \n",
       "2              3    1         0       0        0    0        0      1    0   \n",
       "3              5    1         0       0        0    0        0      1    0   \n",
       "4              0    2         0       0        0    0        0      0    0   \n",
       "\n",
       "   income  consumption  shebao  \n",
       "0       0            0       0  \n",
       "1       0            0       0  \n",
       "2       0            0       0  \n",
       "3       0            0       0  \n",
       "4       0            0       0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[categorical_features].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa76d9bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clicked_products_0009</th>\n",
       "      <th>clicked_products_date_0009</th>\n",
       "      <th>sms_sent_products_0009</th>\n",
       "      <th>sms_sent_products_1019</th>\n",
       "      <th>sms_sent_products_date_0009</th>\n",
       "      <th>sms_sent_products_date_1019</th>\n",
       "      <th>called_products_0009</th>\n",
       "      <th>called_products_1019</th>\n",
       "      <th>called_products_date_0009</th>\n",
       "      <th>called_products_date_1019</th>\n",
       "      <th>...</th>\n",
       "      <th>keypress_120</th>\n",
       "      <th>rule_name_30</th>\n",
       "      <th>rule_name_60</th>\n",
       "      <th>rule_name_90</th>\n",
       "      <th>rule_name_120</th>\n",
       "      <th>semantic_30</th>\n",
       "      <th>semantic_60</th>\n",
       "      <th>semantic_90</th>\n",
       "      <th>semantic_120</th>\n",
       "      <th>model_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>...</td>\n",
       "      <td>小助理 运营商提示音</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>baotai27_其他</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>144 145 140 143</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>IYBPAZX_ZTKMF_OPPOJX_BT IYBPAZX_ZTKMF_OPPOJX_BT</td>\n",
       "      <td>nan</td>\n",
       "      <td>2D 4D</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>baotai27_G</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>7 9 140 150 151 7 131 131</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FQL IYBPAZX_ZMF_CS_BT NWZAZX_ZNWZAMF_NEWOPPO_B...</td>\n",
       "      <td>12D 12D 15D 24D 30D 32D 36D 38D 41D 44D</td>\n",
       "      <td>IYBPAZX_ZTKMF_OPPOJX_BT NWZAZX_ZNWZAMF_NEWOPPO...</td>\n",
       "      <td>IYBPAZX_ZTKMF_OPPOJX_BT IYBPAZX_ZMF_CS_BT IYBP...</td>\n",
       "      <td>2D 4D 8D 10D 12D 15D 20D 24D 26D 28D</td>\n",
       "      <td>30D 32D 34D 36D 39D 43D 45D 47D 51D 53D</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>...</td>\n",
       "      <td>触发发短信 sendMessage_special 没输入手机号 输入手机号2 输入手机号3...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>yingdian888_B jiyonghua661_B huirong888_D ying...</td>\n",
       "      <td>mayi02_D baotai14_B mayi02_D mayi02_D</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>2240 2572 2672 2313 2345 2672 2313 2313 2313 2...</td>\n",
       "      <td>23 24 46 3921 35 21 21 2576 2181 2189 24 23 23...</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>IYBPAZX_ZTKMF_OPPOJX_BT IYBPAZX_ZMF_CS_BT IYBP...</td>\n",
       "      <td>nan</td>\n",
       "      <td>2D 4D 6D</td>\n",
       "      <td>nan</td>\n",
       "      <td>IYBPAZX_TKMF_GD_BZ_GZH_WH IYBPAZX_TKMF_GD_BZ_G...</td>\n",
       "      <td>nan</td>\n",
       "      <td>1D 4D 7D 52D 55D 58D</td>\n",
       "      <td>nan</td>\n",
       "      <td>...</td>\n",
       "      <td>触发发短信 sendMessage_special</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>baotai27_D</td>\n",
       "      <td>baotai27_其他</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               clicked_products_0009  \\\n",
       "0                                                nan   \n",
       "1                                                nan   \n",
       "2                                                nan   \n",
       "3  FQL IYBPAZX_ZMF_CS_BT NWZAZX_ZNWZAMF_NEWOPPO_B...   \n",
       "4                                                nan   \n",
       "\n",
       "                clicked_products_date_0009  \\\n",
       "0                                      nan   \n",
       "1                                      nan   \n",
       "2                                      nan   \n",
       "3  12D 12D 15D 24D 30D 32D 36D 38D 41D 44D   \n",
       "4                                      nan   \n",
       "\n",
       "                              sms_sent_products_0009  \\\n",
       "0                                                nan   \n",
       "1    IYBPAZX_ZTKMF_OPPOJX_BT IYBPAZX_ZTKMF_OPPOJX_BT   \n",
       "2                                                nan   \n",
       "3  IYBPAZX_ZTKMF_OPPOJX_BT NWZAZX_ZNWZAMF_NEWOPPO...   \n",
       "4  IYBPAZX_ZTKMF_OPPOJX_BT IYBPAZX_ZMF_CS_BT IYBP...   \n",
       "\n",
       "                              sms_sent_products_1019  \\\n",
       "0                                                nan   \n",
       "1                                                nan   \n",
       "2                                                nan   \n",
       "3  IYBPAZX_ZTKMF_OPPOJX_BT IYBPAZX_ZMF_CS_BT IYBP...   \n",
       "4                                                nan   \n",
       "\n",
       "            sms_sent_products_date_0009  \\\n",
       "0                                   nan   \n",
       "1                                 2D 4D   \n",
       "2                                   nan   \n",
       "3  2D 4D 8D 10D 12D 15D 20D 24D 26D 28D   \n",
       "4                              2D 4D 6D   \n",
       "\n",
       "               sms_sent_products_date_1019  \\\n",
       "0                                      nan   \n",
       "1                                      nan   \n",
       "2                                      nan   \n",
       "3  30D 32D 34D 36D 39D 43D 45D 47D 51D 53D   \n",
       "4                                      nan   \n",
       "\n",
       "                                called_products_0009 called_products_1019  \\\n",
       "0                                                nan                  nan   \n",
       "1                                                nan                  nan   \n",
       "2                                                nan                  nan   \n",
       "3                                                nan                  nan   \n",
       "4  IYBPAZX_TKMF_GD_BZ_GZH_WH IYBPAZX_TKMF_GD_BZ_G...                  nan   \n",
       "\n",
       "  called_products_date_0009 called_products_date_1019  ...  \\\n",
       "0                       nan                       nan  ...   \n",
       "1                       nan                       nan  ...   \n",
       "2                       nan                       nan  ...   \n",
       "3                       nan                       nan  ...   \n",
       "4      1D 4D 7D 52D 55D 58D                       nan  ...   \n",
       "\n",
       "                                        keypress_120 rule_name_30  \\\n",
       "0                                         小助理 运营商提示音          nan   \n",
       "1                                                nan          nan   \n",
       "2                                                nan          nan   \n",
       "3  触发发短信 sendMessage_special 没输入手机号 输入手机号2 输入手机号3...          nan   \n",
       "4                          触发发短信 sendMessage_special          nan   \n",
       "\n",
       "  rule_name_60                                       rule_name_90  \\\n",
       "0          nan                                                nan   \n",
       "1          nan                                                nan   \n",
       "2          nan                                         baotai27_G   \n",
       "3          nan  yingdian888_B jiyonghua661_B huirong888_D ying...   \n",
       "4          nan                                         baotai27_D   \n",
       "\n",
       "                           rule_name_120 semantic_30 semantic_60  \\\n",
       "0                            baotai27_其他         nan         nan   \n",
       "1                                    nan         nan         nan   \n",
       "2                                    nan         nan         nan   \n",
       "3  mayi02_D baotai14_B mayi02_D mayi02_D         nan         nan   \n",
       "4                            baotai27_其他         nan         nan   \n",
       "\n",
       "                                         semantic_90  \\\n",
       "0                                                nan   \n",
       "1                                                nan   \n",
       "2                          7 9 140 150 151 7 131 131   \n",
       "3  2240 2572 2672 2313 2345 2672 2313 2313 2313 2...   \n",
       "4                                                nan   \n",
       "\n",
       "                                        semantic_120 model_value  \n",
       "0                                    144 145 140 143         nan  \n",
       "1                                                nan         nan  \n",
       "2                                                nan         nan  \n",
       "3  23 24 46 3921 35 21 21 2576 2181 2189 24 23 23...         nan  \n",
       "4                                                nan         nan  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[text_features].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1b3a5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(feature_type):\n",
    "    words = train_df[feature_type].dropna().str.split(' ')\n",
    "    exploded_words = words.explode()\n",
    "    vocabulary = exploded_words.value_counts()\n",
    "    vocabulary = vocabulary[vocabulary > 5]\n",
    "    vocab_size = vocabulary.shape[0]\n",
    "    vocabulary = vocabulary.index.tolist()\n",
    "\n",
    "    return vocabulary, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b542d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {}\n",
    "for type in text_feature_types:\n",
    "    vocab_dict[type] = get_vocabulary(type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99e7e417",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_dict = {col: vocab_info[1]+100 for col, vocab_info in vocab_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9818728c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'products': 273,\n",
       " 'keypress': 207,\n",
       " 'rules': 328,\n",
       " 'semantics': 351,\n",
       " 'insurances': 141,\n",
       " 'model_value': 102}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd701547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'../../data/dicts/token_dict.pkl', 'rb') as f:\n",
    "    token_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e60349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_dict = {\n",
    "    'called_products_0009': 10,\n",
    "    'called_products_1019': 6,\n",
    "    'called_products_date_0009': 10,\n",
    "    'called_products_date_1019': 6,\n",
    "    'clicked_products_0009': 3,\n",
    "    'clicked_products_date_0009': 3,\n",
    "    'keypress_120': 3,\n",
    "    'keypress_30': 3,\n",
    "    'keypress_60': 3,\n",
    "    'keypress_90': 3,\n",
    "    'label_0009': 3,\n",
    "    'label_1019': 3,\n",
    "    'label_date_0009': 3,\n",
    "    'label_date_1019': 3,\n",
    "    'model_value': 3,\n",
    "    'outbound_sent_products_0009': 3,\n",
    "    'outbound_sent_products_date_0009': 3,\n",
    "    'picked_products_0009': 3,\n",
    "    'picked_products_date_0009': 3,\n",
    "    'rule_name_120': 3,\n",
    "    'rule_name_30': 3,\n",
    "    'rule_name_60': 3,\n",
    "    'rule_name_90': 3,\n",
    "    'semantic_120': 5,\n",
    "    'semantic_30': 3,\n",
    "    'semantic_60': 3,\n",
    "    'semantic_90': 5,\n",
    "    'set_all_ins_host_180': 5,\n",
    "    'set_all_ins_host_360': 4,\n",
    "    'sms_sent_products_0009': 10,\n",
    "    'sms_sent_products_1019': 9,\n",
    "    'sms_sent_products_date_0009': 10,\n",
    "    'sms_sent_products_date_1019': 9\n",
    " }\n",
    "\n",
    "category_counts_dict = {col: train_df[col].nunique() for col in categorical_features}\n",
    "\n",
    "\n",
    "def get_text_feature_type(col):\n",
    "    if 'set_' in col:\n",
    "        text_feature_type = 'insurances'\n",
    "    elif 'host_' in col:\n",
    "        text_feature_type = 'hosts'\n",
    "    elif 'products_' in col:\n",
    "        text_feature_type = 'products'\n",
    "    elif 'label_' in col:\n",
    "        text_feature_type = 'labels'\n",
    "    elif 'keypress_' in col:\n",
    "        text_feature_type = 'keypress'\n",
    "    elif 'rule_name_' in col:\n",
    "        text_feature_type = 'rules'\n",
    "    elif 'semantic_' in col:\n",
    "        text_feature_type = 'semantics'\n",
    "    else:\n",
    "        text_feature_type = 'model_value'\n",
    "    return text_feature_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77afc4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore numerical features\n",
    "restored_raw = (\n",
    "    train_df[\"numerical_features\"]\n",
    "    .astype(str)\n",
    "    .str.replace(\"\\n\", \" \", regex=False)\n",
    "    .str.strip(\"[]\")\n",
    "    .str.strip()\n",
    "    .str.split(r\"\\s+\", expand=True)\n",
    "    .apply(lambda col: pd.to_numeric(col.replace('', pd.NA), errors='coerce'))\n",
    "    .fillna(0)\n",
    "    .to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "131b7ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_train_cro = train_df['label'].str.strip(\"[]\").str.split(\" \", expand=True).astype(int).to_numpy()\n",
    "\n",
    "# X\n",
    "X_train = {'numerical_features': restored_raw}\n",
    "\n",
    "for col in categorical_features:\n",
    "    X_train[col] = np.array(train_df[col])\n",
    "\n",
    "for col in text_features:\n",
    "    with open(f'../../data/text_features/{col}.pkl', 'rb') as f:\n",
    "        X_train[col] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6bb553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 拆分索引\n",
    "indices = np.arange(len(y_train_cro))\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42, stratify=y_train_cro)\n",
    "\n",
    "# 拆分 y\n",
    "y_train_split = y_train_cro[train_idx]\n",
    "y_test_split = y_train_cro[test_idx]\n",
    "\n",
    "# 拆分 X\n",
    "X_train_split = {}\n",
    "X_test_split = {}\n",
    "for key in X_train:\n",
    "    arr = X_train[key]\n",
    "    X_train_split[key] = arr[train_idx]\n",
    "    X_test_split[key] = arr[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4fb5821",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_dim, factor_order, activation=None, linear_regularizer=None, factor_regularizer=None,\n",
    "                 **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(FMLayer, self).__init__(**kwargs)\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.factor_order = factor_order\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.linear_regularizer = tf.keras.regularizers.get(linear_regularizer)\n",
    "        self.factor_regularizer = tf.keras.regularizers.get(factor_regularizer)\n",
    "        self.input_spec = tf.keras.layers.InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "\n",
    "        self.input_spec = tf.keras.layers.InputSpec(dtype=tf.keras.backend.floatx(), shape=(None, input_dim))\n",
    "\n",
    "        self.b = self.add_weight(name='bias', shape=(self.output_dim,), initializer='zeros', trainable=True)\n",
    "        self.w = self.add_weight(name='one', shape=(input_dim, self.output_dim), initializer='glorot_uniform',\n",
    "                                 trainable=True, regularizer=self.linear_regularizer)\n",
    "        self.v = self.add_weight(name='two', shape=(input_dim, self.factor_order), initializer='glorot_uniform',\n",
    "                                 trainable=True, regularizer=self.factor_regularizer)\n",
    "\n",
    "        super(FMLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        xw = tf.keras.backend.dot(inputs, self.w) + self.b\n",
    "        \n",
    "        xx = tf.keras.backend.square(inputs)\n",
    "        xv = tf.keras.backend.square(tf.keras.backend.dot(inputs, self.v))\n",
    "        p = 0.5 * tf.keras.backend.sum(xv - tf.keras.backend.dot(xx, tf.keras.backend.square(self.v)), 1)\n",
    "        rp = tf.keras.backend.repeat_elements(tf.keras.backend.reshape(p, (-1, 1)), self.output_dim, axis=-1)\n",
    "\n",
    "        f = xw + rp\n",
    "\n",
    "        output = tf.keras.backend.reshape(f, (-1, self.output_dim))\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.output_dim\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return base_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0a68bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_fm_model(output_units=1, dnn_hidden_units=[1024, 512, 256],\n",
    "           dropout_rate_dnn_input=0.1, dropout_rate_dnn_logit=0.1, l2_reg_dnn=0.0,\n",
    "           fm_factor_order=16, dropout_rate_fm_input=0.1, dropout_rate_fm_logit=0.1,\n",
    "           l2_reg_fm_linear=0.0, l2_reg_fm_factor=0.0,\n",
    "           l2_reg_text_embedding=0.0, l2_reg_categorical_embedding=0.0):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # text inputs\n",
    "    text_inputs = [tf.keras.layers.Input(shape=(max_len_dict[col],), name=col) for col in text_features]\n",
    "    text_embeddings = []\n",
    "    for i, col in enumerate(text_features):\n",
    "        text_feature_type = get_text_feature_type(col)\n",
    "        text_embeddings.append(\n",
    "            tf.keras.layers.Embedding(vocab_size_dict[text_feature_type]+2,\n",
    "                                      int(np.log1p(vocab_size_dict[text_feature_type]) + 2),\n",
    "                                      embeddings_regularizer=tf.keras.regularizers.l2(l2_reg_text_embedding),\n",
    "                                      name=col + '_embed')(text_inputs[i])\n",
    "        )\n",
    "        \n",
    "    text_logit = tf.keras.layers.Concatenate(name='text_concat')(\n",
    "        [tf.keras.layers.GlobalAveragePooling1D()(text_emb) for text_emb in text_embeddings]\n",
    "    )\n",
    "\n",
    "    # categorical inputs\n",
    "    categorical_inputs = [tf.keras.layers.Input(shape=(1,), name=col) for col in categorical_features]\n",
    "    categorical_embeddings = []\n",
    "    for i, col in enumerate(categorical_features):\n",
    "        categorical_embeddings.append(\n",
    "            tf.keras.layers.Embedding(category_counts_dict[col]+2, int(np.log1p(category_counts_dict[col]) + 2),\n",
    "                                      embeddings_regularizer=tf.keras.regularizers.l2(l2_reg_categorical_embedding),\n",
    "                                      name=col + '_embed')(categorical_inputs[i])\n",
    "        )\n",
    "\n",
    "    categorical_logit = tf.keras.layers.Concatenate(name='categorical_concat')(\n",
    "        [tf.keras.layers.Flatten()(cat_emb) for cat_emb in categorical_embeddings]\n",
    "    )\n",
    "\n",
    "    # numerical inputs\n",
    "    numerical_input = tf.keras.layers.Input(shape=(230,), name='numerical_features')\n",
    "\n",
    "    # dnn\n",
    "    dnn_input = tf.keras.layers.Concatenate(name='deep_concat')([text_logit, categorical_logit, numerical_input])\n",
    "    dnn_logit = tf.keras.layers.Dropout(dropout_rate_dnn_input)(dnn_input)\n",
    "    for n_unit in dnn_hidden_units:\n",
    "        dnn_logit = tf.keras.layers.Dense(n_unit, activation='relu',\n",
    "                                          kernel_regularizer=tf.keras.regularizers.l2(l2_reg_dnn))(dnn_logit)\n",
    "        dnn_logit = tf.keras.layers.Dropout(dropout_rate_dnn_logit)(dnn_logit)\n",
    "\n",
    "    # fm\n",
    "    fm_input = tf.keras.layers.Concatenate(name='fm_concat')([text_logit, categorical_logit, numerical_input])\n",
    "    fm_logit = tf.keras.layers.Dropout(dropout_rate_fm_input)(fm_input)\n",
    "    fm_logit = FMLayer(1, fm_factor_order,\n",
    "                        linear_regularizer=tf.keras.regularizers.l2(l2_reg_fm_linear),\n",
    "                        factor_regularizer=tf.keras.regularizers.l2(l2_reg_fm_factor))(fm_logit)\n",
    "    fm_logit = tf.keras.layers.Dropout(dropout_rate_fm_logit)(fm_logit)\n",
    "\n",
    "    outputs = tf.keras.layers.Concatenate()([dnn_logit, fm_logit])\n",
    "    outputs = tf.keras.layers.Dense(output_units, activation='sigmoid')(outputs)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=text_inputs + categorical_inputs + [numerical_input], outputs=outputs, name='DeepFM')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9cc2dbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "n_epochs = 100\n",
    "batch_size = 1024\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.4, patience=1, min_lr=0.00005, verbose=1)\n",
    "callbacks = [es, reduce_lr]\n",
    "\n",
    "\n",
    "def train_model(x_train, y_train, x_test, y_test):\n",
    "    output_units = 3\n",
    "    model = deep_fm_model(output_units)\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy(name='acc'), tf.keras.metrics.AUC(name='auc')],\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr))\n",
    "    \n",
    "    model.fit(x_train, y_train, validation_data=(x_test, y_test),\n",
    "              epochs=n_epochs, batch_size=batch_size, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7f28d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 423ms/step - acc: 0.7828 - auc: 0.8405 - loss: 0.5918 - val_acc: 0.9526 - val_auc: 0.9376 - val_loss: 0.3916 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - acc: 0.9520 - auc: 0.9372 - loss: 0.3262 - val_acc: 0.9540 - val_auc: 0.9386 - val_loss: 0.2510 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 110ms/step - acc: 0.9537 - auc: 0.9394 - loss: 0.2311 - val_acc: 0.9541 - val_auc: 0.9417 - val_loss: 0.2016 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step - acc: 0.9535 - auc: 0.9435 - loss: 0.2020 - val_acc: 0.9541 - val_auc: 0.9471 - val_loss: 0.1844 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 109ms/step - acc: 0.9536 - auc: 0.9491 - loss: 0.1874 - val_acc: 0.9540 - val_auc: 0.9518 - val_loss: 0.1744 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - acc: 0.9536 - auc: 0.9527 - loss: 0.1775 - val_acc: 0.9541 - val_auc: 0.9544 - val_loss: 0.1691 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 112ms/step - acc: 0.9535 - auc: 0.9548 - loss: 0.1723 - val_acc: 0.9540 - val_auc: 0.9550 - val_loss: 0.1668 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 108ms/step - acc: 0.9527 - auc: 0.9551 - loss: 0.1703 - val_acc: 0.9540 - val_auc: 0.9560 - val_loss: 0.1655 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 110ms/step - acc: 0.9531 - auc: 0.9568 - loss: 0.1691 - val_acc: 0.9540 - val_auc: 0.9575 - val_loss: 0.1643 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 109ms/step - acc: 0.9529 - auc: 0.9583 - loss: 0.1674 - val_acc: 0.9541 - val_auc: 0.9582 - val_loss: 0.1634 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 128ms/step - acc: 0.9533 - auc: 0.9591 - loss: 0.1647 - val_acc: 0.9538 - val_auc: 0.9587 - val_loss: 0.1624 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 132ms/step - acc: 0.9528 - auc: 0.9603 - loss: 0.1633 - val_acc: 0.9540 - val_auc: 0.9592 - val_loss: 0.1618 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 113ms/step - acc: 0.9530 - auc: 0.9621 - loss: 0.1611 - val_acc: 0.9540 - val_auc: 0.9601 - val_loss: 0.1611 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 128ms/step - acc: 0.9538 - auc: 0.9633 - loss: 0.1582 - val_acc: 0.9541 - val_auc: 0.9606 - val_loss: 0.1605 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 113ms/step - acc: 0.9532 - auc: 0.9632 - loss: 0.1592 - val_acc: 0.9541 - val_auc: 0.9607 - val_loss: 0.1604 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - acc: 0.9549 - auc: 0.9637 - loss: 0.1556\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 123ms/step - acc: 0.9536 - auc: 0.9642 - loss: 0.1574 - val_acc: 0.9541 - val_auc: 0.9611 - val_loss: 0.1603 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - acc: 0.9533 - auc: 0.9650 - loss: 0.1564 - val_acc: 0.9541 - val_auc: 0.9612 - val_loss: 0.1602 - learning_rate: 5.0000e-05\n",
      "Epoch 18/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 112ms/step - acc: 0.9533 - auc: 0.9656 - loss: 0.1554 - val_acc: 0.9541 - val_auc: 0.9614 - val_loss: 0.1601 - learning_rate: 5.0000e-05\n",
      "Epoch 19/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 115ms/step - acc: 0.9536 - auc: 0.9665 - loss: 0.1542 - val_acc: 0.9541 - val_auc: 0.9615 - val_loss: 0.1600 - learning_rate: 5.0000e-05\n",
      "Epoch 20/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 113ms/step - acc: 0.9536 - auc: 0.9670 - loss: 0.1538 - val_acc: 0.9541 - val_auc: 0.9616 - val_loss: 0.1599 - learning_rate: 5.0000e-05\n",
      "Epoch 21/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 110ms/step - acc: 0.9540 - auc: 0.9671 - loss: 0.1527 - val_acc: 0.9540 - val_auc: 0.9617 - val_loss: 0.1600 - learning_rate: 5.0000e-05\n",
      "Epoch 22/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 111ms/step - acc: 0.9540 - auc: 0.9672 - loss: 0.1528 - val_acc: 0.9538 - val_auc: 0.9615 - val_loss: 0.1602 - learning_rate: 5.0000e-05\n"
     ]
    }
   ],
   "source": [
    "train_model(X_train_split, y_train_split, X_test_split, y_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b036965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c1fa41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
